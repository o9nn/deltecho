(*
 * TokenizerConfig.zpp - Tokenizer Configuration State and Invariants
 * Z++ Formal Specification for Deltecho Model Package
 * 
 * Defines the formal specification for tokenizer configuration,
 * including vocabulary management, special tokens, and encoding settings.
 *)

SCHEMA TokenizerConfig
IMPORTS Types

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 1: Tokenizer Type Definitions
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE TokenizerType ::=
    BPE |                              (* Byte-Pair Encoding *)
    WordPiece |                        (* WordPiece tokenization *)
    Unigram |                          (* Unigram language model *)
    SentencePiece                      (* SentencePiece unified *)

TYPE NormalizerType ::=
    NFD |                              (* Unicode NFD normalization *)
    NFC |                              (* Unicode NFC normalization *)
    NFKD |                             (* Unicode NFKD normalization *)
    NFKC |                             (* Unicode NFKC normalization *)
    Lowercase |                        (* Lowercase conversion *)
    StripAccents |                     (* Remove diacritical marks *)
    None                               (* No normalization *)

TYPE PreTokenizerType ::=
    Whitespace |                       (* Split on whitespace *)
    ByteLevel |                        (* Byte-level BPE *)
    Metaspace |                        (* SentencePiece style *)
    Punctuation |                      (* Split on punctuation *)
    Digits |                           (* Split on digits *)
    None                               (* No pre-tokenization *)

TYPE PaddingStrategy ::=
    Longest |                          (* Pad to longest in batch *)
    MaxLength |                        (* Pad to max_length *)
    DoNotPad                           (* No padding *)

TYPE TruncationStrategy ::=
    LongestFirst |                     (* Truncate longest sequence *)
    OnlyFirst |                        (* Truncate first sequence *)
    OnlySecond |                       (* Truncate second sequence *)
    DoNotTruncate                      (* No truncation *)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 2: Special Tokens Configuration
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE SpecialToken == âŸ¦
    content: seqâ‚ CHAR,                (* Token string, non-empty *)
    id: TokenId,                       (* Token ID in vocabulary *)
    single_word: ğ”¹,                    (* Is single word token *)
    lstrip: ğ”¹,                         (* Strip left whitespace *)
    rstrip: ğ”¹,                         (* Strip right whitespace *)
    normalized: ğ”¹                      (* Apply normalization *)
âŸ§

TYPE SpecialTokensMap == âŸ¦
    pad_token: SpecialToken,
    unk_token: SpecialToken,
    bos_token: SpecialToken,
    eos_token: SpecialToken,
    sep_token: optional SpecialToken,
    cls_token: optional SpecialToken,
    mask_token: optional SpecialToken,
    (* Cognitive stream tokens *)
    stream_1_token: optional SpecialToken,
    stream_2_token: optional SpecialToken,
    stream_3_token: optional SpecialToken,
    pivot_token: optional SpecialToken,
    afford_token: optional SpecialToken,
    salience_token: optional SpecialToken,
    (* Additional special tokens *)
    additional_special_tokens: seq SpecialToken
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 3: Tokenizer Configuration State
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

STATE TokenizerConfigState == âŸ¦
    (* Core configuration *)
    tokenizer_type: TokenizerType,
    vocab_size: VocabSize,
    model_max_length: SeqLen,
    
    (* Normalization and pre-tokenization *)
    normalizer: NormalizerType,
    pre_tokenizer: PreTokenizerType,
    
    (* Special tokens *)
    special_tokens: SpecialTokensMap,
    
    (* Padding configuration *)
    padding_side: {Left, Right},
    padding_strategy: PaddingStrategy,
    pad_to_multiple_of: optional â„•â‚,
    
    (* Truncation configuration *)
    truncation_side: {Left, Right},
    truncation_strategy: TruncationStrategy,
    
    (* Encoding options *)
    add_bos_token: ğ”¹,
    add_eos_token: ğ”¹,
    clean_up_tokenization_spaces: ğ”¹,
    split_special_tokens: ğ”¹,
    
    (* Byte-level BPE specific *)
    byte_fallback: ğ”¹,
    add_prefix_space: ğ”¹,
    
    (* Chat template *)
    chat_template: optional seq CHAR,
    
    (* Cognitive architecture integration *)
    cognitive_tokens_enabled: ğ”¹,
    stream_interleaving: ğ”¹
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 4: Invariant Predicates
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

PRED ValidSpecialToken(t: SpecialToken, vocab_size: VocabSize) â‰œ
    #t.content > 0 âˆ§
    t.id < vocab_size

PRED ValidSpecialTokensMap(m: SpecialTokensMap, vocab_size: VocabSize) â‰œ
    ValidSpecialToken(m.pad_token, vocab_size) âˆ§
    ValidSpecialToken(m.unk_token, vocab_size) âˆ§
    ValidSpecialToken(m.bos_token, vocab_size) âˆ§
    ValidSpecialToken(m.eos_token, vocab_size) âˆ§
    (* All token IDs must be unique *)
    m.pad_token.id â‰  m.unk_token.id âˆ§
    m.pad_token.id â‰  m.bos_token.id âˆ§
    m.pad_token.id â‰  m.eos_token.id âˆ§
    m.unk_token.id â‰  m.bos_token.id âˆ§
    m.unk_token.id â‰  m.eos_token.id âˆ§
    m.bos_token.id â‰  m.eos_token.id

PRED ValidCognitiveTokens(m: SpecialTokensMap, vocab_size: VocabSize) â‰œ
    (m.stream_1_token â‰  âŠ¥ â‡’ ValidSpecialToken(m.stream_1_token!, vocab_size)) âˆ§
    (m.stream_2_token â‰  âŠ¥ â‡’ ValidSpecialToken(m.stream_2_token!, vocab_size)) âˆ§
    (m.stream_3_token â‰  âŠ¥ â‡’ ValidSpecialToken(m.stream_3_token!, vocab_size)) âˆ§
    (m.pivot_token â‰  âŠ¥ â‡’ ValidSpecialToken(m.pivot_token!, vocab_size)) âˆ§
    (m.afford_token â‰  âŠ¥ â‡’ ValidSpecialToken(m.afford_token!, vocab_size)) âˆ§
    (m.salience_token â‰  âŠ¥ â‡’ ValidSpecialToken(m.salience_token!, vocab_size))

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 5: State Invariants
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

INVARIANT TokenizerConfigInvariant
    âˆ€ config: TokenizerConfigState â€¢
        (* Vocabulary size constraints *)
        ValidVocabSize(config.vocab_size) âˆ§
        
        (* Sequence length constraints *)
        ValidSeqLen(config.model_max_length) âˆ§
        
        (* Special tokens validity *)
        ValidSpecialTokensMap(config.special_tokens, config.vocab_size) âˆ§
        
        (* Cognitive tokens validity when enabled *)
        (config.cognitive_tokens_enabled â‡’ 
            ValidCognitiveTokens(config.special_tokens, config.vocab_size)) âˆ§
        
        (* Padding multiple constraint *)
        (config.pad_to_multiple_of â‰  âŠ¥ â‡’ config.pad_to_multiple_of! > 0) âˆ§
        
        (* Stream interleaving requires cognitive tokens *)
        (config.stream_interleaving â‡’ config.cognitive_tokens_enabled)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 6: Default Configuration
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

CONST DEFAULT_TOKENIZER_CONFIG: TokenizerConfigState = âŸ¦
    tokenizer_type â†¦ BPE,
    vocab_size â†¦ DEFAULT_VOCAB_SIZE,
    model_max_length â†¦ DEFAULT_SEQ_LEN,
    normalizer â†¦ NFC,
    pre_tokenizer â†¦ ByteLevel,
    special_tokens â†¦ âŸ¦
        pad_token â†¦ âŸ¦content â†¦ "<pad>", id â†¦ PAD_TOKEN_ID, 
                     single_word â†¦ true, lstrip â†¦ false, rstrip â†¦ false, normalized â†¦ falseâŸ§,
        unk_token â†¦ âŸ¦content â†¦ "<unk>", id â†¦ UNK_TOKEN_ID,
                     single_word â†¦ true, lstrip â†¦ false, rstrip â†¦ false, normalized â†¦ falseâŸ§,
        bos_token â†¦ âŸ¦content â†¦ "<s>", id â†¦ BOS_TOKEN_ID,
                     single_word â†¦ true, lstrip â†¦ false, rstrip â†¦ false, normalized â†¦ falseâŸ§,
        eos_token â†¦ âŸ¦content â†¦ "</s>", id â†¦ EOS_TOKEN_ID,
                     single_word â†¦ true, lstrip â†¦ false, rstrip â†¦ false, normalized â†¦ falseâŸ§,
        sep_token â†¦ âŠ¥,
        cls_token â†¦ âŠ¥,
        mask_token â†¦ âŠ¥,
        stream_1_token â†¦ âŸ¦content â†¦ "<|stream1|>", id â†¦ STREAM_1_TOKEN_ID,
                         single_word â†¦ true, lstrip â†¦ false, rstrip â†¦ false, normalized â†¦ falseâŸ§,
        stream_2_token â†¦ âŸ¦content â†¦ "<|stream2|>", id â†¦ STREAM_2_TOKEN_ID,
                         single_word â†¦ true, lstrip â†¦ false, rstrip â†¦ false, normalized â†¦ falseâŸ§,
        stream_3_token â†¦ âŸ¦content â†¦ "<|stream3|>", id â†¦ STREAM_3_TOKEN_ID,
                         single_word â†¦ true, lstrip â†¦ false, rstrip â†¦ false, normalized â†¦ falseâŸ§,
        pivot_token â†¦ âŸ¦content â†¦ "<|pivot|>", id â†¦ PIVOT_TOKEN_ID,
                       single_word â†¦ true, lstrip â†¦ false, rstrip â†¦ false, normalized â†¦ falseâŸ§,
        afford_token â†¦ âŸ¦content â†¦ "<|afford|>", id â†¦ AFFORD_TOKEN_ID,
                        single_word â†¦ true, lstrip â†¦ false, rstrip â†¦ false, normalized â†¦ falseâŸ§,
        salience_token â†¦ âŸ¦content â†¦ "<|salience|>", id â†¦ SALIENCE_TOKEN_ID,
                          single_word â†¦ true, lstrip â†¦ false, rstrip â†¦ false, normalized â†¦ falseâŸ§,
        additional_special_tokens â†¦ âŸ¨âŸ©
    âŸ§,
    padding_side â†¦ Right,
    padding_strategy â†¦ Longest,
    pad_to_multiple_of â†¦ âŠ¥,
    truncation_side â†¦ Right,
    truncation_strategy â†¦ LongestFirst,
    add_bos_token â†¦ true,
    add_eos_token â†¦ true,
    clean_up_tokenization_spaces â†¦ true,
    split_special_tokens â†¦ false,
    byte_fallback â†¦ true,
    add_prefix_space â†¦ false,
    chat_template â†¦ âŠ¥,
    cognitive_tokens_enabled â†¦ true,
    stream_interleaving â†¦ true
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 7: Configuration Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP SetVocabSize
    Î”STATE TokenizerConfigState
    new_vocab_size?: VocabSize
    
    PRE ValidVocabSize(new_vocab_size?)
    POST vocab_size' = new_vocab_size?

OP SetModelMaxLength
    Î”STATE TokenizerConfigState
    new_max_length?: SeqLen
    
    PRE ValidSeqLen(new_max_length?)
    POST model_max_length' = new_max_length?

OP EnableCognitiveTokens
    Î”STATE TokenizerConfigState
    
    PRE Â¬cognitive_tokens_enabled
    POST cognitive_tokens_enabled' = true âˆ§
         ValidCognitiveTokens(special_tokens', vocab_size')

OP DisableCognitiveTokens
    Î”STATE TokenizerConfigState
    
    PRE cognitive_tokens_enabled
    POST cognitive_tokens_enabled' = false âˆ§
         stream_interleaving' = false

OP AddSpecialToken
    Î”STATE TokenizerConfigState
    new_token?: SpecialToken
    
    PRE ValidSpecialToken(new_token?, vocab_size) âˆ§
        new_token?.id âˆ‰ {t.id | t âˆˆ ran special_tokens.additional_special_tokens}
    POST special_tokens'.additional_special_tokens = 
         special_tokens.additional_special_tokens â€ âŸ¨new_token?âŸ©

END TokenizerConfig
