(*
 * Tokenizer.zpp - Tokenization and Detokenization Contracts
 * Z++ Formal Specification for Deltecho Model Package
 * 
 * Defines the formal contracts for tokenization operations,
 * including encoding, decoding, and cognitive stream token handling.
 *)

SCHEMA Tokenizer
IMPORTS Types, TokenizerConfig

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 1: Input/Output Types
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE TextInput == seq CHAR             (* Raw text input *)
TYPE TokenSequence == seq TokenId      (* Sequence of token IDs *)
TYPE AttentionMask == seq {0, 1}       (* Binary attention mask *)

TYPE EncodedOutput == âŸ¦
    input_ids: TokenSequence,
    attention_mask: AttentionMask,
    token_type_ids: optional seq â„•,
    special_tokens_mask: optional seq {0, 1},
    length: â„•
âŸ§

TYPE BatchEncodedOutput == âŸ¦
    input_ids: seq TokenSequence,
    attention_mask: seq AttentionMask,
    token_type_ids: optional seq (seq â„•),
    special_tokens_mask: optional seq (seq {0, 1}),
    lengths: seq â„•
âŸ§

TYPE DecodedOutput == âŸ¦
    text: TextInput,
    offsets: optional seq (â„• Ã— â„•),     (* Character offsets *)
    tokens: optional seq TextInput      (* Individual token strings *)
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 2: Cognitive Stream Types
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE StreamMarker == âŸ¦
    stream_id: StreamId,
    position: â„•,
    token_id: TokenId
âŸ§

TYPE CognitiveAnnotation == âŸ¦
    stream_markers: seq StreamMarker,
    pivot_positions: seq â„•,
    affordance_positions: seq â„•,
    salience_positions: seq â„•,
    current_step: StepId,
    current_mode: CognitiveMode
âŸ§

TYPE CognitiveEncodedOutput == âŸ¦
    base: EncodedOutput,
    cognitive: CognitiveAnnotation
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 3: Tokenizer State
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

STATE TokenizerState == âŸ¦
    config: TokenizerConfigState,
    vocabulary: TokenId â‡¸ TextInput,   (* Partial function: ID â†’ token string *)
    merges: seq (TextInput Ã— TextInput), (* BPE merge rules *)
    
    (* Cognitive state *)
    cognitive_enabled: ğ”¹,
    current_stream: StreamId,
    current_step: StepId
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 4: Invariant Predicates
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

PRED ValidTokenSequence(tokens: TokenSequence, vocab_size: VocabSize) â‰œ
    âˆ€ t âˆˆ ran tokens â€¢ t < vocab_size

PRED ValidEncodedOutput(output: EncodedOutput, config: TokenizerConfigState) â‰œ
    ValidTokenSequence(output.input_ids, config.vocab_size) âˆ§
    #output.input_ids = #output.attention_mask âˆ§
    output.length = #output.input_ids âˆ§
    output.length â‰¤ config.model_max_length âˆ§
    (output.token_type_ids â‰  âŠ¥ â‡’ #output.token_type_ids! = output.length) âˆ§
    (output.special_tokens_mask â‰  âŠ¥ â‡’ #output.special_tokens_mask! = output.length)

PRED ValidBatchEncodedOutput(output: BatchEncodedOutput, config: TokenizerConfigState) â‰œ
    #output.input_ids = #output.attention_mask âˆ§
    #output.input_ids = #output.lengths âˆ§
    âˆ€ i: 1..#output.input_ids â€¢
        ValidTokenSequence(output.input_ids[i], config.vocab_size) âˆ§
        #output.input_ids[i] = #output.attention_mask[i] âˆ§
        output.lengths[i] = #output.input_ids[i]

PRED ValidCognitiveAnnotation(ann: CognitiveAnnotation, seq_len: â„•) â‰œ
    (* All positions must be within sequence *)
    (âˆ€ m âˆˆ ran ann.stream_markers â€¢ m.position < seq_len) âˆ§
    (âˆ€ p âˆˆ ran ann.pivot_positions â€¢ p < seq_len) âˆ§
    (âˆ€ p âˆˆ ran ann.affordance_positions â€¢ p < seq_len) âˆ§
    (âˆ€ p âˆˆ ran ann.salience_positions â€¢ p < seq_len) âˆ§
    
    (* Step must be valid *)
    1 â‰¤ ann.current_step â‰¤ COGNITIVE_STEPS

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 5: State Invariants
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

INVARIANT TokenizerStateInvariant
    âˆ€ state: TokenizerState â€¢
        (* Vocabulary size must match config *)
        #dom state.vocabulary = state.config.vocab_size âˆ§
        
        (* All special tokens must be in vocabulary *)
        state.config.special_tokens.pad_token.id âˆˆ dom state.vocabulary âˆ§
        state.config.special_tokens.unk_token.id âˆˆ dom state.vocabulary âˆ§
        state.config.special_tokens.bos_token.id âˆˆ dom state.vocabulary âˆ§
        state.config.special_tokens.eos_token.id âˆˆ dom state.vocabulary âˆ§
        
        (* Cognitive state constraints *)
        (state.cognitive_enabled â‡’ state.config.cognitive_tokens_enabled) âˆ§
        1 â‰¤ state.current_stream â‰¤ COGNITIVE_STREAMS âˆ§
        1 â‰¤ state.current_step â‰¤ COGNITIVE_STEPS

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 6: Tokenization Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP Encode
    ÎSTATE TokenizerState
    text?: TextInput
    add_special_tokens?: ğ”¹
    padding?: PaddingStrategy
    truncation?: TruncationStrategy
    max_length?: optional SeqLen
    output!: EncodedOutput
    
    PRE 
        (* Input text can be empty *)
        true
    POST
        (* Output must be valid *)
        ValidEncodedOutput(output!, config) âˆ§
        
        (* BOS token added if configured *)
        (add_special_tokens? âˆ§ config.add_bos_token â‡’ 
            output!.input_ids[1] = config.special_tokens.bos_token.id) âˆ§
        
        (* EOS token added if configured *)
        (add_special_tokens? âˆ§ config.add_eos_token â‡’ 
            output!.input_ids[output!.length] = config.special_tokens.eos_token.id) âˆ§
        
        (* Length respects max_length *)
        (max_length? â‰  âŠ¥ â‡’ output!.length â‰¤ max_length?!) âˆ§
        
        (* Truncation applied if needed *)
        (truncation? â‰  DoNotTruncate âˆ§ max_length? â‰  âŠ¥ â‡’ 
            output!.length â‰¤ max_length?!)

OP EncodeBatch
    ÎSTATE TokenizerState
    texts?: seq TextInput
    add_special_tokens?: ğ”¹
    padding?: PaddingStrategy
    truncation?: TruncationStrategy
    max_length?: optional SeqLen
    output!: BatchEncodedOutput
    
    PRE 
        #texts? > 0
    POST
        ValidBatchEncodedOutput(output!, config) âˆ§
        #output!.input_ids = #texts? âˆ§
        
        (* All sequences same length if padding *)
        (padding? â‰  DoNotPad â‡’ 
            âˆ€ i, j: 1..#output!.input_ids â€¢ 
                #output!.input_ids[i] = #output!.input_ids[j])

OP Decode
    ÎSTATE TokenizerState
    token_ids?: TokenSequence
    skip_special_tokens?: ğ”¹
    clean_up_tokenization_spaces?: ğ”¹
    output!: DecodedOutput
    
    PRE 
        ValidTokenSequence(token_ids?, config.vocab_size)
    POST
        (* All tokens must map to strings *)
        âˆ€ t âˆˆ ran token_ids? â€¢ 
            (skip_special_tokens? âˆ§ IsSpecialToken(t) â‡’ true) âˆ¨
            t âˆˆ dom vocabulary

OP DecodeBatch
    ÎSTATE TokenizerState
    batch_token_ids?: seq TokenSequence
    skip_special_tokens?: ğ”¹
    clean_up_tokenization_spaces?: ğ”¹
    outputs!: seq DecodedOutput
    
    PRE 
        #batch_token_ids? > 0 âˆ§
        âˆ€ tokens âˆˆ ran batch_token_ids? â€¢ 
            ValidTokenSequence(tokens, config.vocab_size)
    POST
        #outputs! = #batch_token_ids?

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 7: Cognitive Stream Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP EncodeWithCognitive
    Î”STATE TokenizerState
    text?: TextInput
    stream_id?: StreamId
    step_id?: StepId
    output!: CognitiveEncodedOutput
    
    PRE 
        cognitive_enabled âˆ§
        1 â‰¤ stream_id? â‰¤ COGNITIVE_STREAMS âˆ§
        1 â‰¤ step_id? â‰¤ COGNITIVE_STEPS
    POST
        ValidEncodedOutput(output!.base, config) âˆ§
        ValidCognitiveAnnotation(output!.cognitive, output!.base.length) âˆ§
        
        (* Stream marker inserted *)
        âˆƒ m âˆˆ ran output!.cognitive.stream_markers â€¢ 
            m.stream_id = stream_id? âˆ§
            m.token_id = GetStreamToken(stream_id?) âˆ§
        
        (* State updated *)
        current_stream' = stream_id? âˆ§
        current_step' = step_id?

OP InsertPivotToken
    Î”STATE TokenizerState
    tokens?: TokenSequence
    position?: â„•
    output!: TokenSequence
    
    PRE 
        cognitive_enabled âˆ§
        position? â‰¤ #tokens?
    POST
        #output! = #tokens? + 1 âˆ§
        output![position? + 1] = config.special_tokens.pivot_token!.id

OP InsertAffordanceToken
    Î”STATE TokenizerState
    tokens?: TokenSequence
    position?: â„•
    output!: TokenSequence
    
    PRE 
        cognitive_enabled âˆ§
        position? â‰¤ #tokens?
    POST
        #output! = #tokens? + 1 âˆ§
        output![position? + 1] = config.special_tokens.afford_token!.id

OP InsertSalienceToken
    Î”STATE TokenizerState
    tokens?: TokenSequence
    position?: â„•
    output!: TokenSequence
    
    PRE 
        cognitive_enabled âˆ§
        position? â‰¤ #tokens?
    POST
        #output! = #tokens? + 1 âˆ§
        output![position? + 1] = config.special_tokens.salience_token!.id

OP AdvanceCognitiveStep
    Î”STATE TokenizerState
    
    PRE cognitive_enabled
    POST
        current_step' = ((current_step - 1 + 1) mod COGNITIVE_STEPS) + 1 âˆ§
        (* Stream advances every PHASE_OFFSET steps *)
        (current_step' mod PHASE_OFFSET = 1 â‡’ 
            current_stream' = ((current_stream - 1 + 1) mod COGNITIVE_STREAMS) + 1)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 8: Helper Functions
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

FUNC IsSpecialToken(t: TokenId): ğ”¹ â‰œ
    t = config.special_tokens.pad_token.id âˆ¨
    t = config.special_tokens.unk_token.id âˆ¨
    t = config.special_tokens.bos_token.id âˆ¨
    t = config.special_tokens.eos_token.id âˆ¨
    (config.special_tokens.stream_1_token â‰  âŠ¥ âˆ§ t = config.special_tokens.stream_1_token!.id) âˆ¨
    (config.special_tokens.stream_2_token â‰  âŠ¥ âˆ§ t = config.special_tokens.stream_2_token!.id) âˆ¨
    (config.special_tokens.stream_3_token â‰  âŠ¥ âˆ§ t = config.special_tokens.stream_3_token!.id) âˆ¨
    (config.special_tokens.pivot_token â‰  âŠ¥ âˆ§ t = config.special_tokens.pivot_token!.id) âˆ¨
    (config.special_tokens.afford_token â‰  âŠ¥ âˆ§ t = config.special_tokens.afford_token!.id) âˆ¨
    (config.special_tokens.salience_token â‰  âŠ¥ âˆ§ t = config.special_tokens.salience_token!.id)

FUNC GetStreamToken(stream: StreamId): TokenId â‰œ
    CASE stream OF
        1 â†’ config.special_tokens.stream_1_token!.id
        2 â†’ config.special_tokens.stream_2_token!.id
        3 â†’ config.special_tokens.stream_3_token!.id
    END

FUNC GetCurrentMode(step: StepId): CognitiveMode â‰œ
    IF step â‰¤ EXPRESSIVE_STEPS THEN Expressive ELSE Reflective

FUNC GetCurrentPhase(stream: StreamId, step: StepId): CognitivePhase â‰œ
    LET offset = (stream - 1) * PHASE_OFFSET IN
    LET adjusted_step = ((step - 1 + offset) mod COGNITIVE_STEPS) + 1 IN
    CASE adjusted_step mod 4 OF
        1 â†’ Perception
        2 â†’ Action
        3 â†’ Simulation
        0 â†’ Simulation  (* Step 12 maps to Simulation *)
    END

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 9: Roundtrip Properties
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

THEOREM EncodeDecodeRoundtrip:
    âˆ€ text: TextInput; state: TokenizerState â€¢
        LET encoded = Encode(text, false, DoNotPad, DoNotTruncate, âŠ¥) IN
        LET decoded = Decode(encoded.input_ids, false, false) IN
        decoded.text = text

THEOREM BatchConsistency:
    âˆ€ texts: seq TextInput; state: TokenizerState â€¢
        LET batch = EncodeBatch(texts, true, DoNotPad, DoNotTruncate, âŠ¥) IN
        âˆ€ i: 1..#texts â€¢
            LET single = Encode(texts[i], true, DoNotPad, DoNotTruncate, âŠ¥) IN
            batch.input_ids[i] = single.input_ids

END Tokenizer
