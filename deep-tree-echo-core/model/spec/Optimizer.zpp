(*
 * Optimizer.zpp - Optimization Contracts for Training Pipeline
 * Z++ Formal Specification for Deltecho Model Package
 * 
 * Defines the formal contracts for gradient computation, parameter updates,
 * learning rate scheduling, and cognitive-aware optimization strategies.
 *)

SCHEMA Optimizer
IMPORTS Types, ModelConfig, Model, LossFunction

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 1: Optimizer Type Definitions
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE LearningRate == â„                 (* Positive learning rate *)
TYPE Momentum == â„                     (* Momentum coefficient [0, 1) *)
TYPE WeightDecay == â„                  (* Weight decay coefficient â‰¥ 0 *)
TYPE GradientNorm == â„                 (* Non-negative gradient norm *)
TYPE StepCount == â„•                    (* Optimization step counter *)

TYPE OptimizerType ::=
    SGD |                              (* Stochastic Gradient Descent *)
    Adam |                             (* Adam optimizer *)
    AdamW |                            (* Adam with decoupled weight decay *)
    LAMB |                             (* Layer-wise Adaptive Moments *)
    Adafactor |                        (* Memory-efficient Adam variant *)
    Lion |                             (* Evolved Sign Momentum *)
    CognitiveAdam                      (* Cognitive-aware Adam variant *)

TYPE SchedulerType ::=
    Constant |                         (* Fixed learning rate *)
    Linear |                           (* Linear warmup/decay *)
    Cosine |                           (* Cosine annealing *)
    CosineWithRestarts |               (* Cosine with warm restarts *)
    Polynomial |                       (* Polynomial decay *)
    InverseSqrt |                      (* Inverse square root decay *)
    OneCycle |                         (* One-cycle policy *)
    CognitiveAdaptive                  (* Cognitive-aware scheduling *)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 2: Optimizer Configuration Types
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE AdamConfig == âŸ¦
    lr: LearningRate,
    beta1: Momentum,
    beta2: Momentum,
    eps: â„,
    weight_decay: WeightDecay,
    amsgrad: ğ”¹,
    fused: ğ”¹
âŸ§

TYPE SGDConfig == âŸ¦
    lr: LearningRate,
    momentum: Momentum,
    dampening: â„,
    weight_decay: WeightDecay,
    nesterov: ğ”¹
âŸ§

TYPE LionConfig == âŸ¦
    lr: LearningRate,
    beta1: Momentum,
    beta2: Momentum,
    weight_decay: WeightDecay
âŸ§

TYPE CognitiveAdamConfig == âŸ¦
    (* Base Adam parameters *)
    lr: LearningRate,
    beta1: Momentum,
    beta2: Momentum,
    eps: â„,
    weight_decay: WeightDecay,
    
    (* Cognitive-specific parameters *)
    stream_lr_multipliers: Tensor1D[COGNITIVE_STREAMS],
    mode_lr_multipliers: âŸ¦expressive: â„, reflective: â„âŸ§,
    triad_momentum_scaling: ğ”¹,
    nested_shell_decay: ğ”¹,
    nest_decay_factors: Tensor1D[4],
    
    (* Cross-stream gradient coordination *)
    cross_stream_gradient_sync: ğ”¹,
    stream_gradient_weights: Tensor1D[COGNITIVE_STREAMS]
âŸ§

TYPE SchedulerConfig == âŸ¦
    scheduler_type: SchedulerType,
    
    (* Warmup *)
    warmup_steps: â„•,
    warmup_ratio: optional Probability,
    warmup_type: {linear, exponential, constant},
    
    (* Decay *)
    total_steps: â„•,
    min_lr: LearningRate,
    min_lr_ratio: optional Probability,
    
    (* Cosine-specific *)
    num_cycles: optional â„•â‚,
    cycle_mult: optional â„,
    
    (* Polynomial-specific *)
    power: optional â„,
    
    (* Cognitive-specific *)
    cognitive_phase_scaling: optional ğ”¹,
    phase_lr_factors: optional Tensor1D[COGNITIVE_STEPS]
âŸ§

TYPE GradientConfig == âŸ¦
    (* Gradient clipping *)
    max_grad_norm: optional GradientNorm,
    max_grad_value: optional â„,
    
    (* Gradient accumulation *)
    accumulation_steps: â„•â‚,
    
    (* Mixed precision *)
    fp16: ğ”¹,
    bf16: ğ”¹,
    loss_scale: optional â„,
    dynamic_loss_scaling: ğ”¹,
    
    (* Gradient checkpointing *)
    gradient_checkpointing: ğ”¹,
    checkpoint_layers: optional seq â„•
âŸ§

TYPE OptimizerConfig == âŸ¦
    optimizer_type: OptimizerType,
    adam_config: optional AdamConfig,
    sgd_config: optional SGDConfig,
    lion_config: optional LionConfig,
    cognitive_adam_config: optional CognitiveAdamConfig,
    scheduler_config: SchedulerConfig,
    gradient_config: GradientConfig,
    
    (* Parameter groups *)
    param_groups: seq âŸ¦
        name: seq CHAR,
        params: seq seq CHAR,
        lr_mult: â„,
        weight_decay_mult: â„
    âŸ§
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 3: Gradient and Parameter Types
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE Gradient == âŸ¦
    param_name: seq CHAR,
    grad: Tensor,
    shape: seq â„•â‚,
    dtype: {float16, bfloat16, float32}
âŸ§

TYPE ParameterState == âŸ¦
    param_name: seq CHAR,
    param: Tensor,
    grad: optional Tensor,
    
    (* Optimizer state *)
    exp_avg: optional Tensor,          (* First moment estimate *)
    exp_avg_sq: optional Tensor,       (* Second moment estimate *)
    max_exp_avg_sq: optional Tensor,   (* AMSGrad max *)
    momentum_buffer: optional Tensor,  (* SGD momentum *)
    
    (* Cognitive state *)
    stream_gradients: optional seq Tensor,
    mode_gradients: optional âŸ¦expressive: Tensor, reflective: TensorâŸ§,
    triad_momentum: optional Tensor1D[4]
âŸ§

TYPE GradientAccumulator == âŸ¦
    accumulated_grads: seq Gradient,
    accumulation_count: â„•,
    target_accumulation: â„•â‚,
    is_ready: ğ”¹
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 4: Optimizer State
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

STATE OptimizerState == âŸ¦
    config: OptimizerConfig,
    
    (* Step tracking *)
    global_step: StepCount,
    local_step: StepCount,
    epoch: â„•,
    
    (* Learning rate state *)
    current_lr: LearningRate,
    base_lr: LearningRate,
    
    (* Parameter states *)
    param_states: seq ParameterState,
    param_groups: seq âŸ¦
        name: seq CHAR,
        lr: LearningRate,
        weight_decay: WeightDecay
    âŸ§,
    
    (* Gradient accumulation *)
    accumulator: GradientAccumulator,
    
    (* Mixed precision state *)
    loss_scale: â„,
    scale_growth_tracker: â„•,
    
    (* Cognitive optimization state *)
    cognitive_enabled: ğ”¹,
    current_stream: StreamId,
    current_step: StepId,
    stream_update_counts: Tensor1D[COGNITIVE_STREAMS],
    mode_update_counts: âŸ¦expressive: â„•, reflective: â„•âŸ§,
    
    (* Statistics *)
    grad_norm_history: seq GradientNorm,
    lr_history: seq LearningRate,
    loss_scale_history: seq â„
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 5: Invariant Predicates
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

PRED ValidAdamConfig(c: AdamConfig) â‰œ
    c.lr > 0 âˆ§
    0 â‰¤ c.beta1 < 1 âˆ§
    0 â‰¤ c.beta2 < 1 âˆ§
    c.eps > 0 âˆ§
    c.weight_decay â‰¥ 0

PRED ValidSchedulerConfig(c: SchedulerConfig) â‰œ
    c.total_steps > 0 âˆ§
    c.warmup_steps â‰¤ c.total_steps âˆ§
    c.min_lr â‰¥ 0 âˆ§
    (c.warmup_ratio â‰  âŠ¥ â‡’ 0 â‰¤ c.warmup_ratio! â‰¤ 1) âˆ§
    (c.num_cycles â‰  âŠ¥ â‡’ c.num_cycles! > 0) âˆ§
    (c.power â‰  âŠ¥ â‡’ c.power! > 0)

PRED ValidGradientConfig(c: GradientConfig) â‰œ
    c.accumulation_steps > 0 âˆ§
    (c.max_grad_norm â‰  âŠ¥ â‡’ c.max_grad_norm! > 0) âˆ§
    (c.loss_scale â‰  âŠ¥ â‡’ c.loss_scale! > 0) âˆ§
    Â¬(c.fp16 âˆ§ c.bf16)  (* Cannot use both *)

PRED ValidOptimizerConfig(c: OptimizerConfig) â‰œ
    ValidSchedulerConfig(c.scheduler_config) âˆ§
    ValidGradientConfig(c.gradient_config) âˆ§
    (c.optimizer_type = Adam âˆ¨ c.optimizer_type = AdamW â‡’
        c.adam_config â‰  âŠ¥ âˆ§ ValidAdamConfig(c.adam_config!)) âˆ§
    (c.optimizer_type = CognitiveAdam â‡’
        c.cognitive_adam_config â‰  âŠ¥)

PRED ValidParameterState(p: ParameterState, opt_type: OptimizerType) â‰œ
    (opt_type âˆˆ {Adam, AdamW, CognitiveAdam} â‡’
        p.exp_avg â‰  âŠ¥ âˆ§ p.exp_avg_sq â‰  âŠ¥) âˆ§
    (opt_type = SGD â‡’ true)

PRED ValidGradientAccumulator(a: GradientAccumulator) â‰œ
    a.accumulation_count â‰¤ a.target_accumulation âˆ§
    (a.is_ready â‡” a.accumulation_count = a.target_accumulation)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 6: State Invariants
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

INVARIANT OptimizerStateInvariant
    âˆ€ state: OptimizerState â€¢
        ValidOptimizerConfig(state.config) âˆ§
        state.current_lr > 0 âˆ§
        state.base_lr > 0 âˆ§
        state.loss_scale > 0 âˆ§
        ValidGradientAccumulator(state.accumulator) âˆ§
        
        (* Cognitive state consistency *)
        (state.cognitive_enabled â‡’
            1 â‰¤ state.current_stream â‰¤ COGNITIVE_STREAMS âˆ§
            1 â‰¤ state.current_step â‰¤ COGNITIVE_STEPS) âˆ§
        
        (* History lengths bounded *)
        #state.grad_norm_history â‰¤ 1000 âˆ§
        #state.lr_history â‰¤ 1000

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 7: Gradient Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP ComputeGradients
    Î”STATE OptimizerState
    loss?: LossValue
    gradients!: seq Gradient
    
    PRE loss? â‰¥ 0
    POST
        #gradients! = #param_states âˆ§
        (âˆ€ i: 1..#gradients! â€¢ 
            gradients![i].param_name = param_states[i].param_name âˆ§
            gradients![i].shape = shape(param_states[i].param))

OP AccumulateGradients
    Î”STATE OptimizerState
    gradients?: seq Gradient
    
    PRE 
        #gradients? = #param_states âˆ§
        accumulator.accumulation_count < accumulator.target_accumulation
    POST
        accumulator'.accumulation_count = accumulator.accumulation_count + 1 âˆ§
        
        (* Gradients accumulated *)
        (âˆ€ i: 1..#gradients? â€¢
            accumulator'.accumulated_grads[i].grad = 
                accumulator.accumulated_grads[i].grad + gradients?[i].grad) âˆ§
        
        (* Ready flag updated *)
        accumulator'.is_ready = 
            (accumulator'.accumulation_count = accumulator'.target_accumulation)

OP AverageAccumulatedGradients
    Î”STATE OptimizerState
    averaged_gradients!: seq Gradient
    
    PRE accumulator.is_ready
    POST
        #averaged_gradients! = #accumulator.accumulated_grads âˆ§
        (âˆ€ i: 1..#averaged_gradients! â€¢
            averaged_gradients![i].grad = 
                accumulator.accumulated_grads[i].grad / accumulator.target_accumulation)

OP ClipGradientsByNorm
    Î”STATE OptimizerState
    gradients?: seq Gradient
    max_norm?: GradientNorm
    clipped_gradients!: seq Gradient
    total_norm!: GradientNorm
    
    PRE max_norm? > 0
    POST
        (* Compute total gradient norm *)
        total_norm! = sqrt(Î£áµ¢ normÂ²(gradients?[i].grad)) âˆ§
        
        (* Clip if necessary *)
        (total_norm! > max_norm? â‡’
            âˆ€ i: 1..#clipped_gradients! â€¢
                clipped_gradients![i].grad = 
                    gradients?[i].grad * (max_norm? / total_norm!)) âˆ§
        (total_norm! â‰¤ max_norm? â‡’
            clipped_gradients! = gradients?) âˆ§
        
        (* History updated *)
        grad_norm_history' = grad_norm_history â€ âŸ¨total_norm!âŸ©

OP ClipGradientsByValue
    ÎSTATE OptimizerState
    gradients?: seq Gradient
    max_value?: â„
    clipped_gradients!: seq Gradient
    
    PRE max_value? > 0
    POST
        #clipped_gradients! = #gradients? âˆ§
        (âˆ€ i: 1..#clipped_gradients!; j: 1..size(gradients?[i].grad) â€¢
            clipped_gradients![i].grad[j] = 
                clamp(gradients?[i].grad[j], -max_value?, max_value?))

OP ScaleGradientsForMixedPrecision
    Î”STATE OptimizerState
    gradients?: seq Gradient
    scaled_gradients!: seq Gradient
    
    PRE config.gradient_config.fp16 âˆ¨ config.gradient_config.bf16
    POST
        #scaled_gradients! = #gradients? âˆ§
        (âˆ€ i: 1..#scaled_gradients! â€¢
            scaled_gradients![i].grad = gradients?[i].grad / loss_scale)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 8: Parameter Update Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP AdamStep
    Î”STATE OptimizerState
    param_idx?: â„•â‚
    grad?: Tensor
    
    PRE 
        param_idx? â‰¤ #param_states âˆ§
        config.optimizer_type âˆˆ {Adam, AdamW}
    POST
        LET p = param_states[param_idx?] IN
        LET cfg = config.adam_config! IN
        LET t = global_step + 1 IN
        
        (* Update biased first moment estimate *)
        p'.exp_avg! = cfg.beta1 * p.exp_avg! + (1 - cfg.beta1) * grad? âˆ§
        
        (* Update biased second moment estimate *)
        p'.exp_avg_sq! = cfg.beta2 * p.exp_avg_sq! + (1 - cfg.beta2) * grad?Â² âˆ§
        
        (* Bias correction *)
        LET m_hat = p'.exp_avg! / (1 - cfg.beta1^t) IN
        LET v_hat = p'.exp_avg_sq! / (1 - cfg.beta2^t) IN
        
        (* Parameter update *)
        (config.optimizer_type = AdamW â‡’
            (* Decoupled weight decay *)
            p'.param = p.param * (1 - current_lr * cfg.weight_decay) - 
                       current_lr * m_hat / (sqrt(v_hat) + cfg.eps)) âˆ§
        (config.optimizer_type = Adam â‡’
            (* L2 regularization in gradient *)
            p'.param = p.param - 
                       current_lr * (m_hat / (sqrt(v_hat) + cfg.eps) + 
                                    cfg.weight_decay * p.param))

OP SGDStep
    Î”STATE OptimizerState
    param_idx?: â„•â‚
    grad?: Tensor
    
    PRE 
        param_idx? â‰¤ #param_states âˆ§
        config.optimizer_type = SGD
    POST
        LET p = param_states[param_idx?] IN
        LET cfg = config.sgd_config! IN
        
        (* Update momentum buffer *)
        (cfg.momentum > 0 â‡’
            p'.momentum_buffer! = cfg.momentum * p.momentum_buffer! + 
                                  (1 - cfg.dampening) * grad?) âˆ§
        
        (* Compute update direction *)
        LET d = (cfg.momentum > 0 ? 
                    (cfg.nesterov ? grad? + cfg.momentum * p'.momentum_buffer! 
                                 : p'.momentum_buffer!)
                    : grad?) IN
        
        (* Parameter update with weight decay *)
        p'.param = p.param - current_lr * (d + cfg.weight_decay * p.param)

OP LionStep
    Î”STATE OptimizerState
    param_idx?: â„•â‚
    grad?: Tensor
    
    PRE 
        param_idx? â‰¤ #param_states âˆ§
        config.optimizer_type = Lion
    POST
        LET p = param_states[param_idx?] IN
        LET cfg = config.lion_config! IN
        
        (* Compute update using interpolation and sign *)
        LET update = sign(cfg.beta1 * p.exp_avg! + (1 - cfg.beta1) * grad?) IN
        
        (* Update momentum *)
        p'.exp_avg! = cfg.beta2 * p.exp_avg! + (1 - cfg.beta2) * grad? âˆ§
        
        (* Parameter update with decoupled weight decay *)
        p'.param = p.param * (1 - current_lr * cfg.weight_decay) - 
                   current_lr * update

OP CognitiveAdamStep
    Î”STATE OptimizerState
    param_idx?: â„•â‚
    grad?: Tensor
    stream_id?: StreamId
    step_id?: StepId
    
    PRE 
        param_idx? â‰¤ #param_states âˆ§
        config.optimizer_type = CognitiveAdam âˆ§
        1 â‰¤ stream_id? â‰¤ COGNITIVE_STREAMS âˆ§
        1 â‰¤ step_id? â‰¤ COGNITIVE_STEPS
    POST
        LET p = param_states[param_idx?] IN
        LET cfg = config.cognitive_adam_config! IN
        LET t = global_step + 1 IN
        
        (* Stream-specific learning rate *)
        LET stream_lr = current_lr * cfg.stream_lr_multipliers[stream_id?] IN
        
        (* Mode-specific learning rate *)
        LET mode = GetModeForStep(step_id?) IN
        LET mode_lr = stream_lr * 
            (mode = Expressive ? cfg.mode_lr_multipliers.expressive 
                              : cfg.mode_lr_multipliers.reflective) IN
        
        (* Triad momentum scaling *)
        LET triad = GetTriadForStep(step_id?) IN
        LET scaled_beta1 = (cfg.triad_momentum_scaling ?
            cfg.beta1 * (1 + 0.1 * (triad - 1)) : cfg.beta1) IN
        
        (* Update moments with cognitive scaling *)
        p'.exp_avg! = scaled_beta1 * p.exp_avg! + (1 - scaled_beta1) * grad? âˆ§
        p'.exp_avg_sq! = cfg.beta2 * p.exp_avg_sq! + (1 - cfg.beta2) * grad?Â² âˆ§
        
        (* Nested shell decay *)
        LET nest_level = GetNestLevel(step_id?) IN
        LET decay_factor = (cfg.nested_shell_decay ?
            cfg.nest_decay_factors[nest_level] : 1.0) IN
        
        (* Bias-corrected estimates *)
        LET m_hat = p'.exp_avg! / (1 - scaled_beta1^t) IN
        LET v_hat = p'.exp_avg_sq! / (1 - cfg.beta2^t) IN
        
        (* Parameter update *)
        p'.param = p.param * (1 - mode_lr * cfg.weight_decay * decay_factor) - 
                   mode_lr * m_hat / (sqrt(v_hat) + cfg.eps) âˆ§
        
        (* Update cognitive statistics *)
        stream_update_counts'[stream_id?] = stream_update_counts[stream_id?] + 1 âˆ§
        (mode = Expressive â‡’ 
            mode_update_counts'.expressive = mode_update_counts.expressive + 1) âˆ§
        (mode = Reflective â‡’ 
            mode_update_counts'.reflective = mode_update_counts.reflective + 1)

OP Step
    Î”STATE OptimizerState
    
    PRE accumulator.is_ready
    POST
        (* All parameters updated *)
        (âˆ€ i: 1..#param_states â€¢ 
            param_states'[i].param â‰  param_states[i].param) âˆ§
        
        (* Step counters incremented *)
        global_step' = global_step + 1 âˆ§
        local_step' = local_step + 1 âˆ§
        
        (* Accumulator reset *)
        accumulator'.accumulation_count = 0 âˆ§
        accumulator'.is_ready = false âˆ§
        (âˆ€ i: 1..#accumulator'.accumulated_grads â€¢
            accumulator'.accumulated_grads[i].grad = zeros(shape))

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 9: Learning Rate Scheduling Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP GetLearningRate
    ÎSTATE OptimizerState
    step?: StepCount
    lr!: LearningRate
    
    PRE step? â‰¤ config.scheduler_config.total_steps
    POST
        lr! > 0 âˆ§
        lr! â‰¥ config.scheduler_config.min_lr âˆ§
        
        CASE config.scheduler_config.scheduler_type OF
            Constant â†’ lr! = base_lr
            Linear â†’ lr! = LinearSchedule(step?, base_lr, config.scheduler_config)
            Cosine â†’ lr! = CosineSchedule(step?, base_lr, config.scheduler_config)
            CosineWithRestarts â†’ 
                lr! = CosineWithRestartsSchedule(step?, base_lr, config.scheduler_config)
            Polynomial â†’ 
                lr! = PolynomialSchedule(step?, base_lr, config.scheduler_config)
            InverseSqrt â†’ 
                lr! = InverseSqrtSchedule(step?, base_lr, config.scheduler_config)
            OneCycle â†’ 
                lr! = OneCycleSchedule(step?, base_lr, config.scheduler_config)
            CognitiveAdaptive â†’ 
                lr! = CognitiveAdaptiveSchedule(step?, base_lr, config.scheduler_config,
                                                current_stream, current_step)
        END

OP UpdateLearningRate
    Î”STATE OptimizerState
    
    PRE true
    POST
        current_lr' = GetLearningRate(global_step) âˆ§
        lr_history' = lr_history â€ âŸ¨current_lr'âŸ© âˆ§
        
        (* Update all parameter groups *)
        (âˆ€ i: 1..#param_groups' â€¢
            param_groups'[i].lr = current_lr' * config.param_groups[i].lr_mult)

OP WarmupLearningRate
    ÎSTATE OptimizerState
    step?: StepCount
    warmup_lr!: LearningRate
    
    PRE step? < config.scheduler_config.warmup_steps
    POST
        CASE config.scheduler_config.warmup_type OF
            linear â†’ 
                warmup_lr! = base_lr * (step? + 1) / config.scheduler_config.warmup_steps
            exponential â†’ 
                warmup_lr! = base_lr * (step? + 1)^2 / config.scheduler_config.warmup_steps^2
            constant â†’ 
                warmup_lr! = base_lr * 0.1
        END

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 10: Mixed Precision Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP UpdateLossScale
    Î”STATE OptimizerState
    gradients_finite?: ğ”¹
    
    PRE config.gradient_config.dynamic_loss_scaling
    POST
        (gradients_finite? â‡’
            (* Increase scale after successful steps *)
            scale_growth_tracker' = scale_growth_tracker + 1 âˆ§
            (scale_growth_tracker' â‰¥ 2000 â‡’
                loss_scale' = min(loss_scale * 2, 2^16) âˆ§
                scale_growth_tracker' = 0)) âˆ§
        (Â¬gradients_finite? â‡’
            (* Decrease scale on overflow *)
            loss_scale' = max(loss_scale / 2, 1) âˆ§
            scale_growth_tracker' = 0) âˆ§
        
        loss_scale_history' = loss_scale_history â€ âŸ¨loss_scale'âŸ©

OP CheckGradientOverflow
    ÎSTATE OptimizerState
    gradients?: seq Gradient
    is_finite!: ğ”¹
    
    PRE config.gradient_config.fp16 âˆ¨ config.gradient_config.bf16
    POST
        is_finite! = âˆ€ g âˆˆ ran gradients? â€¢ 
            âˆ€ v âˆˆ elements(g.grad) â€¢ isfinite(v)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 11: Cognitive Optimization Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP SyncCrossStreamGradients
    Î”STATE OptimizerState
    stream_gradients?: seq seq Gradient
    synced_gradients!: seq Gradient
    
    PRE 
        cognitive_enabled âˆ§
        config.cognitive_adam_config â‰  âŠ¥ âˆ§
        config.cognitive_adam_config!.cross_stream_gradient_sync âˆ§
        #stream_gradients? = COGNITIVE_STREAMS
    POST
        #synced_gradients! = #stream_gradients?[1] âˆ§
        
        (* Weighted average of stream gradients *)
        (âˆ€ i: 1..#synced_gradients! â€¢
            synced_gradients![i].grad = 
                Î£â±¼ config.cognitive_adam_config!.stream_gradient_weights[j] * 
                    stream_gradients?[j][i].grad)

OP AdvanceCognitiveState
    Î”STATE OptimizerState
    
    PRE cognitive_enabled
    POST
        current_step' = ((current_step - 1 + 1) mod COGNITIVE_STEPS) + 1 âˆ§
        (current_step' mod PHASE_OFFSET = 1 â‡’
            current_stream' = ((current_stream - 1 + 1) mod COGNITIVE_STREAMS) + 1)

OP GetCognitiveScaledLR
    ÎSTATE OptimizerState
    base_lr?: LearningRate
    stream_id?: StreamId
    step_id?: StepId
    scaled_lr!: LearningRate
    
    PRE 
        cognitive_enabled âˆ§
        config.cognitive_adam_config â‰  âŠ¥ âˆ§
        1 â‰¤ stream_id? â‰¤ COGNITIVE_STREAMS âˆ§
        1 â‰¤ step_id? â‰¤ COGNITIVE_STEPS
    POST
        LET cfg = config.cognitive_adam_config! IN
        LET stream_mult = cfg.stream_lr_multipliers[stream_id?] IN
        LET mode = GetModeForStep(step_id?) IN
        LET mode_mult = (mode = Expressive ? 
            cfg.mode_lr_multipliers.expressive : 
            cfg.mode_lr_multipliers.reflective) IN
        scaled_lr! = base_lr? * stream_mult * mode_mult

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 12: Helper Functions
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

FUNC LinearSchedule(step: StepCount, base_lr: LearningRate, cfg: SchedulerConfig): LearningRate â‰œ
    IF step < cfg.warmup_steps THEN
        base_lr * (step + 1) / cfg.warmup_steps
    ELSE
        LET decay_steps = cfg.total_steps - cfg.warmup_steps IN
        LET decay_progress = (step - cfg.warmup_steps) / decay_steps IN
        base_lr * (1 - decay_progress) + cfg.min_lr * decay_progress

FUNC CosineSchedule(step: StepCount, base_lr: LearningRate, cfg: SchedulerConfig): LearningRate â‰œ
    IF step < cfg.warmup_steps THEN
        base_lr * (step + 1) / cfg.warmup_steps
    ELSE
        LET decay_steps = cfg.total_steps - cfg.warmup_steps IN
        LET progress = (step - cfg.warmup_steps) / decay_steps IN
        cfg.min_lr + (base_lr - cfg.min_lr) * (1 + cos(Ï€ * progress)) / 2

FUNC CosineWithRestartsSchedule(step: StepCount, base_lr: LearningRate, 
                                 cfg: SchedulerConfig): LearningRate â‰œ
    LET num_cycles = cfg.num_cycles! IN
    LET cycle_length = cfg.total_steps / num_cycles IN
    LET cycle_step = step mod cycle_length IN
    LET progress = cycle_step / cycle_length IN
    cfg.min_lr + (base_lr - cfg.min_lr) * (1 + cos(Ï€ * progress)) / 2

FUNC PolynomialSchedule(step: StepCount, base_lr: LearningRate, 
                        cfg: SchedulerConfig): LearningRate â‰œ
    IF step < cfg.warmup_steps THEN
        base_lr * (step + 1) / cfg.warmup_steps
    ELSE
        LET decay_steps = cfg.total_steps - cfg.warmup_steps IN
        LET progress = (step - cfg.warmup_steps) / decay_steps IN
        (base_lr - cfg.min_lr) * (1 - progress)^cfg.power! + cfg.min_lr

FUNC InverseSqrtSchedule(step: StepCount, base_lr: LearningRate, 
                         cfg: SchedulerConfig): LearningRate â‰œ
    IF step < cfg.warmup_steps THEN
        base_lr * (step + 1) / cfg.warmup_steps
    ELSE
        base_lr * sqrt(cfg.warmup_steps / (step + 1))

FUNC GetModeForStep(step: StepId): CognitiveMode â‰œ
    IF step âˆˆ {1, 2, 3, 4, 5, 8, 9} THEN Expressive
    ELSE Reflective

FUNC GetTriadForStep(step: StepId): {1, 2, 3, 4} â‰œ
    CASE step OF
        1, 5, 9 â†’ 1
        2, 6, 10 â†’ 2
        3, 7, 11 â†’ 3
        4, 8, 12 â†’ 4
    END

FUNC GetNestLevel(step: StepId): {1, 2, 3, 4} â‰œ
    (* Based on OEIS A000081 nested shell structure *)
    CASE step OF
        1 â†’ 1
        2, 3 â†’ 2
        4, 5, 6, 7 â†’ 3
        8, 9, 10, 11, 12 â†’ 4
    END

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 13: Theorems and Properties
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

THEOREM LearningRateBounded:
    âˆ€ state: OptimizerState; step: StepCount â€¢
        step â‰¤ state.config.scheduler_config.total_steps â‡’
        state.config.scheduler_config.min_lr â‰¤ GetLearningRate(step) â‰¤ state.base_lr

THEOREM GradientClippingBound:
    âˆ€ grads: seq Gradient; max_norm: GradientNorm; clipped: seq Gradient â€¢
        ClipGradientsByNorm(grads, max_norm, clipped) â‡’
        sqrt(Î£áµ¢ normÂ²(clipped[i].grad)) â‰¤ max_norm

THEOREM AdamConvergence:
    (* Under standard assumptions, Adam converges *)
    âˆ€ state: OptimizerState â€¢
        state.config.optimizer_type âˆˆ {Adam, AdamW} âˆ§
        ValidAdamConfig(state.config.adam_config!) â‡’
        (* Regret bound: O(sqrt(T)) *)
        true

THEOREM MomentumPreservation:
    âˆ€ state, state': OptimizerState; grad: Tensor â€¢
        AdamStep(state, state', idx, grad) â‡’
        (* Momentum is exponential moving average *)
        state'.param_states[idx].exp_avg! = 
            state.config.adam_config!.beta1 * state.param_states[idx].exp_avg! +
            (1 - state.config.adam_config!.beta1) * grad

THEOREM CognitiveStreamBalance:
    âˆ€ state: OptimizerState â€¢
        state.cognitive_enabled âˆ§ state.global_step > COGNITIVE_STEPS * 10 â‡’
        (* All streams receive approximately equal updates *)
        âˆ€ sâ‚, sâ‚‚: 1..COGNITIVE_STREAMS â€¢
            |state.stream_update_counts[sâ‚] - state.stream_update_counts[sâ‚‚]| < 
            state.global_step / COGNITIVE_STREAMS

THEOREM WarmupMonotonicity:
    âˆ€ state: OptimizerState; sâ‚, sâ‚‚: StepCount â€¢
        sâ‚ < sâ‚‚ < state.config.scheduler_config.warmup_steps â‡’
        WarmupLearningRate(sâ‚) â‰¤ WarmupLearningRate(sâ‚‚)

END Optimizer
