(*
 * LossFunction.zpp - Loss Function Contracts for Training Pipeline
 * Z++ Formal Specification for Deltecho Model Package
 * 
 * Defines the formal contracts for loss computation, including cross-entropy,
 * cognitive-aware losses, auxiliary losses, and loss aggregation strategies.
 *)

SCHEMA LossFunction
IMPORTS Types, ModelConfig, Model

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 1: Loss Type Definitions
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE LossValue == â„                    (* Non-negative loss value *)
TYPE GradientValue == â„                (* Gradient component *)
TYPE LossWeight == â„                   (* Loss term weight, typically [0, 1] *)

TYPE LossReduction ::=
    None |                             (* Return per-sample losses *)
    Mean |                             (* Average over samples *)
    Sum |                              (* Sum over samples *)
    WeightedMean                       (* Weighted average *)

TYPE LossType ::=
    CrossEntropy |                     (* Standard cross-entropy *)
    LabelSmoothedCE |                  (* Label smoothing cross-entropy *)
    FocalLoss |                        (* Focal loss for imbalanced data *)
    CognitiveLoss |                    (* Cognitive-aware composite loss *)
    ContrastiveLoss |                  (* Contrastive learning loss *)
    KLDivergence |                     (* KL divergence loss *)
    MSELoss                            (* Mean squared error *)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 2: Loss Configuration Types
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE CrossEntropyConfig == âŸ¦
    reduction: LossReduction,
    ignore_index: TokenId,
    label_smoothing: optional â„,
    weight: optional Tensor1D[VocabSize]
âŸ§

TYPE FocalLossConfig == âŸ¦
    reduction: LossReduction,
    alpha: â„,
    gamma: â„,
    ignore_index: TokenId
âŸ§

TYPE CognitiveLossConfig == âŸ¦
    (* Base loss *)
    base_loss_type: LossType,
    base_loss_weight: LossWeight,
    
    (* Stream coherence loss *)
    stream_coherence_enabled: ğ”¹,
    stream_coherence_weight: LossWeight,
    
    (* Mode transition loss *)
    mode_transition_enabled: ğ”¹,
    mode_transition_weight: LossWeight,
    
    (* Relevance realization loss *)
    relevance_loss_enabled: ğ”¹,
    relevance_loss_weight: LossWeight,
    
    (* Cross-stream attention loss *)
    cross_stream_loss_enabled: ğ”¹,
    cross_stream_loss_weight: LossWeight,
    
    (* Nested shell consistency loss *)
    nested_shell_loss_enabled: ğ”¹,
    nested_shell_loss_weight: LossWeight,
    nest_levels: â„•â‚
âŸ§

TYPE AuxiliaryLossConfig == âŸ¦
    (* Router loss for MoE *)
    router_z_loss_enabled: ğ”¹,
    router_z_loss_weight: LossWeight,
    
    (* Load balancing loss *)
    load_balance_loss_enabled: ğ”¹,
    load_balance_loss_weight: LossWeight,
    
    (* Attention entropy loss *)
    attention_entropy_enabled: ğ”¹,
    attention_entropy_weight: LossWeight,
    attention_entropy_target: â„,
    
    (* Hidden state regularization *)
    hidden_state_reg_enabled: ğ”¹,
    hidden_state_reg_weight: LossWeight,
    hidden_state_reg_type: {L1, L2, Spectral}
âŸ§

TYPE LossFunctionConfig == âŸ¦
    primary_loss_type: LossType,
    cross_entropy_config: optional CrossEntropyConfig,
    focal_loss_config: optional FocalLossConfig,
    cognitive_loss_config: optional CognitiveLossConfig,
    auxiliary_loss_config: optional AuxiliaryLossConfig,
    
    (* Gradient scaling *)
    loss_scale: â„,
    gradient_clipping: optional â„,
    
    (* Numerical stability *)
    eps: â„,
    log_softmax_dim: â„¤
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 3: Loss Input/Output Types
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE LossInput == âŸ¦
    (* Model outputs *)
    logits: Tensor3D[BatchSize, SeqLen, VocabSize],
    labels: Tensor2D[BatchSize, SeqLen],
    
    (* Masking *)
    attention_mask: optional Tensor2D[BatchSize, SeqLen],
    loss_mask: optional Tensor2D[BatchSize, SeqLen],
    
    (* Cognitive state *)
    cognitive_hidden_states: optional seq Tensor3D[BatchSize, SeqLen, HiddenDim],
    stream_ids: optional Tensor1D[BatchSize],
    step_ids: optional Tensor1D[BatchSize],
    mode_ids: optional Tensor1D[BatchSize],
    
    (* Auxiliary outputs *)
    router_logits: optional seq Tensor3D[BatchSize, SeqLen, NumExperts],
    attention_weights: optional seq Tensor4D[BatchSize, NumHeads, SeqLen, SeqLen]
âŸ§

TYPE LossOutput == âŸ¦
    (* Primary loss *)
    total_loss: LossValue,
    
    (* Component losses *)
    primary_loss: LossValue,
    auxiliary_loss: optional LossValue,
    cognitive_loss: optional LossValue,
    
    (* Detailed breakdown *)
    loss_components: âŸ¦
        cross_entropy: optional LossValue,
        stream_coherence: optional LossValue,
        mode_transition: optional LossValue,
        relevance_realization: optional LossValue,
        cross_stream_attention: optional LossValue,
        nested_shell_consistency: optional LossValue,
        router_z_loss: optional LossValue,
        load_balance_loss: optional LossValue,
        attention_entropy: optional LossValue,
        hidden_state_reg: optional LossValue
    âŸ§,
    
    (* Metrics *)
    perplexity: â„,
    accuracy: Probability,
    tokens_evaluated: â„•
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 4: Loss Function State
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

STATE LossFunctionState == âŸ¦
    config: LossFunctionConfig,
    
    (* Running statistics *)
    total_loss_accumulated: â„,
    steps_accumulated: â„•,
    tokens_accumulated: â„•,
    
    (* Component statistics *)
    component_losses_accumulated: âŸ¦
        cross_entropy: â„,
        stream_coherence: â„,
        mode_transition: â„,
        relevance_realization: â„,
        cross_stream_attention: â„,
        nested_shell_consistency: â„,
        router_z_loss: â„,
        load_balance_loss: â„
    âŸ§,
    
    (* Cognitive mode statistics *)
    expressive_loss_accumulated: â„,
    reflective_loss_accumulated: â„,
    expressive_tokens: â„•,
    reflective_tokens: â„•
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 5: Invariant Predicates
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

PRED ValidLossConfig(c: LossFunctionConfig) â‰œ
    c.loss_scale > 0 âˆ§
    c.eps > 0 âˆ§
    (c.gradient_clipping â‰  âŠ¥ â‡’ c.gradient_clipping! > 0) âˆ§
    
    (* Cross-entropy config valid *)
    (c.cross_entropy_config â‰  âŠ¥ â‡’
        (c.cross_entropy_config!.label_smoothing â‰  âŠ¥ â‡’
            0 â‰¤ c.cross_entropy_config!.label_smoothing! < 1)) âˆ§
    
    (* Focal loss config valid *)
    (c.focal_loss_config â‰  âŠ¥ â‡’
        c.focal_loss_config!.alpha > 0 âˆ§
        c.focal_loss_config!.gamma â‰¥ 0) âˆ§
    
    (* Cognitive loss weights sum to reasonable value *)
    (c.cognitive_loss_config â‰  âŠ¥ â‡’
        c.cognitive_loss_config!.base_loss_weight > 0 âˆ§
        c.cognitive_loss_config!.stream_coherence_weight â‰¥ 0 âˆ§
        c.cognitive_loss_config!.mode_transition_weight â‰¥ 0 âˆ§
        c.cognitive_loss_config!.relevance_loss_weight â‰¥ 0 âˆ§
        c.cognitive_loss_config!.cross_stream_loss_weight â‰¥ 0 âˆ§
        c.cognitive_loss_config!.nested_shell_loss_weight â‰¥ 0)

PRED ValidLossInput(input: LossInput) â‰œ
    dimâ‚(input.logits) = dimâ‚(input.labels) âˆ§
    dimâ‚‚(input.logits) = dimâ‚‚(input.labels) âˆ§
    (input.attention_mask â‰  âŠ¥ â‡’
        dimâ‚(input.attention_mask!) = dimâ‚(input.labels) âˆ§
        dimâ‚‚(input.attention_mask!) = dimâ‚‚(input.labels)) âˆ§
    (input.loss_mask â‰  âŠ¥ â‡’
        dimâ‚(input.loss_mask!) = dimâ‚(input.labels) âˆ§
        dimâ‚‚(input.loss_mask!) = dimâ‚‚(input.labels)) âˆ§
    (input.stream_ids â‰  âŠ¥ â‡’
        #input.stream_ids! = dimâ‚(input.labels) âˆ§
        âˆ€ s âˆˆ ran input.stream_ids! â€¢ 1 â‰¤ s â‰¤ COGNITIVE_STREAMS) âˆ§
    (input.step_ids â‰  âŠ¥ â‡’
        #input.step_ids! = dimâ‚(input.labels) âˆ§
        âˆ€ s âˆˆ ran input.step_ids! â€¢ 1 â‰¤ s â‰¤ COGNITIVE_STEPS)

PRED ValidLossOutput(output: LossOutput) â‰œ
    output.total_loss â‰¥ 0 âˆ§
    output.primary_loss â‰¥ 0 âˆ§
    (output.auxiliary_loss â‰  âŠ¥ â‡’ output.auxiliary_loss! â‰¥ 0) âˆ§
    (output.cognitive_loss â‰  âŠ¥ â‡’ output.cognitive_loss! â‰¥ 0) âˆ§
    output.perplexity â‰¥ 1 âˆ§
    0 â‰¤ output.accuracy â‰¤ 1 âˆ§
    output.tokens_evaluated > 0

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 6: State Invariants
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

INVARIANT LossFunctionStateInvariant
    âˆ€ state: LossFunctionState â€¢
        ValidLossConfig(state.config) âˆ§
        state.steps_accumulated â‰¥ 0 âˆ§
        state.tokens_accumulated â‰¥ 0 âˆ§
        (state.steps_accumulated > 0 â‡’ state.tokens_accumulated > 0) âˆ§
        state.expressive_tokens + state.reflective_tokens â‰¤ state.tokens_accumulated

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 7: Primary Loss Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP ComputeCrossEntropyLoss
    ÎSTATE LossFunctionState
    logits?: Tensor3D[BatchSize, SeqLen, VocabSize]
    labels?: Tensor2D[BatchSize, SeqLen]
    mask?: optional Tensor2D[BatchSize, SeqLen]
    loss!: LossValue
    per_token_loss!: optional Tensor2D[BatchSize, SeqLen]
    
    PRE 
        dimâ‚(logits?) = dimâ‚(labels?) âˆ§
        dimâ‚‚(logits?) = dimâ‚‚(labels?)
    POST
        loss! â‰¥ 0 âˆ§
        
        (* Loss computed as negative log likelihood *)
        (* loss = -Î£ log(softmax(logits)[label]) / num_tokens *)
        
        (* Masked tokens excluded *)
        (mask? â‰  âŠ¥ â‡’
            per_token_loss! â‰  âŠ¥ âˆ§
            âˆ€ i, j â€¢ mask?![i][j] = 0 â‡’ per_token_loss![i][j] = 0) âˆ§
        
        (* Label smoothing applied if configured *)
        (config.cross_entropy_config â‰  âŠ¥ âˆ§
         config.cross_entropy_config!.label_smoothing â‰  âŠ¥ â‡’
            (* Smoothed distribution: (1-Îµ)Â·one_hot + Îµ/V *)
            true)

OP ComputeLabelSmoothedLoss
    ÎSTATE LossFunctionState
    logits?: Tensor3D[BatchSize, SeqLen, VocabSize]
    labels?: Tensor2D[BatchSize, SeqLen]
    smoothing?: â„
    loss!: LossValue
    
    PRE 
        0 â‰¤ smoothing? < 1 âˆ§
        dimâ‚(logits?) = dimâ‚(labels?)
    POST
        loss! â‰¥ 0 âˆ§
        (* Loss interpolates between CE and uniform distribution *)
        (* loss = (1-Îµ)Â·CE(logits, labels) + ÎµÂ·H(uniform) *)
        true

OP ComputeFocalLoss
    ÎSTATE LossFunctionState
    logits?: Tensor3D[BatchSize, SeqLen, VocabSize]
    labels?: Tensor2D[BatchSize, SeqLen]
    alpha?: â„
    gamma?: â„
    loss!: LossValue
    
    PRE 
        alpha? > 0 âˆ§
        gamma? â‰¥ 0 âˆ§
        dimâ‚(logits?) = dimâ‚(labels?)
    POST
        loss! â‰¥ 0 âˆ§
        (* Focal loss: -Î±Â·(1-p)^Î³Â·log(p) *)
        (* Down-weights well-classified examples *)
        true

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 8: Cognitive Loss Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP ComputeStreamCoherenceLoss
    ÎSTATE LossFunctionState
    hidden_states?: seq Tensor3D[BatchSize, SeqLen, HiddenDim]
    stream_ids?: Tensor1D[BatchSize]
    loss!: LossValue
    
    PRE 
        #hidden_states? = COGNITIVE_STREAMS âˆ§
        âˆ€ s âˆˆ ran stream_ids? â€¢ 1 â‰¤ s â‰¤ COGNITIVE_STREAMS
    POST
        loss! â‰¥ 0 âˆ§
        (* Encourages coherent representations within each stream *)
        (* loss = Î£_stream variance(hidden_states[stream]) *)
        true

OP ComputeModeTransitionLoss
    ÎSTATE LossFunctionState
    hidden_states?: Tensor3D[BatchSize, SeqLen, HiddenDim]
    mode_ids?: Tensor1D[BatchSize]
    loss!: LossValue
    
    PRE 
        âˆ€ m âˆˆ ran mode_ids? â€¢ m âˆˆ {0, 1}  (* 0=expressive, 1=reflective *)
    POST
        loss! â‰¥ 0 âˆ§
        (* Encourages smooth transitions between cognitive modes *)
        (* Penalizes abrupt representation changes at mode boundaries *)
        true

OP ComputeRelevanceRealizationLoss
    ÎSTATE LossFunctionState
    hidden_states?: Tensor3D[BatchSize, SeqLen, HiddenDim]
    step_ids?: Tensor1D[BatchSize]
    pivot_positions?: seq â„•
    loss!: LossValue
    
    PRE 
        âˆ€ s âˆˆ ran step_ids? â€¢ 1 â‰¤ s â‰¤ COGNITIVE_STEPS âˆ§
        âˆ€ p âˆˆ ran pivot_positions? â€¢ p < dimâ‚‚(hidden_states?)
    POST
        loss! â‰¥ 0 âˆ§
        (* Encourages pivotal relevance at designated steps *)
        (* Pivot steps should have high attention to salient features *)
        true

OP ComputeCrossStreamAttentionLoss
    ÎSTATE LossFunctionState
    attention_weights?: seq Tensor4D[BatchSize, NumHeads, SeqLen, SeqLen]
    stream_ids?: Tensor1D[BatchSize]
    loss!: LossValue
    
    PRE 
        #attention_weights? > 0 âˆ§
        âˆ€ s âˆˆ ran stream_ids? â€¢ 1 â‰¤ s â‰¤ COGNITIVE_STREAMS
    POST
        loss! â‰¥ 0 âˆ§
        (* Encourages appropriate cross-stream attention patterns *)
        (* Stream 1 should attend to Stream 2's action *)
        (* Stream 3 should reflect on simulation of action *)
        true

OP ComputeNestedShellConsistencyLoss
    ÎSTATE LossFunctionState
    hidden_states?: seq Tensor3D[BatchSize, SeqLen, HiddenDim]
    nest_levels?: â„•â‚
    loss!: LossValue
    
    PRE 
        nest_levels? â‰¤ 4 âˆ§
        #hidden_states? â‰¥ nest_levels?
    POST
        loss! â‰¥ 0 âˆ§
        (* Enforces OEIS A000081 nested shell structure *)
        (* Level 1: 1 term, Level 2: 2 terms, Level 3: 4 terms, Level 4: 9 terms *)
        (* Encourages hierarchical representation consistency *)
        true

OP ComputeTriadAlignmentLoss
    ÎSTATE LossFunctionState
    hidden_states?: Tensor3D[BatchSize, SeqLen, HiddenDim]
    step_ids?: Tensor1D[BatchSize]
    loss!: LossValue
    
    PRE 
        âˆ€ s âˆˆ ran step_ids? â€¢ 1 â‰¤ s â‰¤ COGNITIVE_STEPS
    POST
        loss! â‰¥ 0 âˆ§
        (* Encourages alignment within triads: {1,5,9}, {2,6,10}, {3,7,11}, {4,8,12} *)
        (* Steps within same triad should have similar representations *)
        true

OP ComputeCognitiveLoss
    ÎSTATE LossFunctionState
    input?: LossInput
    cognitive_loss!: LossValue
    components!: âŸ¦
        stream_coherence: LossValue,
        mode_transition: LossValue,
        relevance_realization: LossValue,
        cross_stream_attention: LossValue,
        nested_shell_consistency: LossValue
    âŸ§
    
    PRE 
        config.cognitive_loss_config â‰  âŠ¥ âˆ§
        input?.cognitive_hidden_states â‰  âŠ¥
    POST
        cognitive_loss! â‰¥ 0 âˆ§
        
        (* Weighted sum of cognitive loss components *)
        cognitive_loss! = 
            config.cognitive_loss_config!.stream_coherence_weight * components!.stream_coherence +
            config.cognitive_loss_config!.mode_transition_weight * components!.mode_transition +
            config.cognitive_loss_config!.relevance_loss_weight * components!.relevance_realization +
            config.cognitive_loss_config!.cross_stream_loss_weight * components!.cross_stream_attention +
            config.cognitive_loss_config!.nested_shell_loss_weight * components!.nested_shell_consistency

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 9: Auxiliary Loss Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP ComputeRouterZLoss
    ÎSTATE LossFunctionState
    router_logits?: seq Tensor3D[BatchSize, SeqLen, NumExperts]
    loss!: LossValue
    
    PRE 
        config.auxiliary_loss_config â‰  âŠ¥ âˆ§
        config.auxiliary_loss_config!.router_z_loss_enabled âˆ§
        #router_logits? > 0
    POST
        loss! â‰¥ 0 âˆ§
        (* Z-loss encourages router logits to stay small *)
        (* loss = mean(log(Î£ exp(router_logits))Â²) *)
        true

OP ComputeLoadBalanceLoss
    ÎSTATE LossFunctionState
    router_logits?: seq Tensor3D[BatchSize, SeqLen, NumExperts]
    loss!: LossValue
    
    PRE 
        config.auxiliary_loss_config â‰  âŠ¥ âˆ§
        config.auxiliary_loss_config!.load_balance_loss_enabled âˆ§
        #router_logits? > 0
    POST
        loss! â‰¥ 0 âˆ§
        (* Encourages uniform expert utilization *)
        (* loss = num_experts Â· Î£(fraction_routed Â· mean_routing_prob) *)
        true

OP ComputeAttentionEntropyLoss
    ÎSTATE LossFunctionState
    attention_weights?: seq Tensor4D[BatchSize, NumHeads, SeqLen, SeqLen]
    target_entropy?: â„
    loss!: LossValue
    
    PRE 
        config.auxiliary_loss_config â‰  âŠ¥ âˆ§
        config.auxiliary_loss_config!.attention_entropy_enabled âˆ§
        #attention_weights? > 0 âˆ§
        target_entropy? > 0
    POST
        loss! â‰¥ 0 âˆ§
        (* Encourages attention entropy close to target *)
        (* loss = |H(attention) - target_entropy|Â² *)
        true

OP ComputeHiddenStateRegularization
    ÎSTATE LossFunctionState
    hidden_states?: Tensor3D[BatchSize, SeqLen, HiddenDim]
    reg_type?: {L1, L2, Spectral}
    loss!: LossValue
    
    PRE 
        config.auxiliary_loss_config â‰  âŠ¥ âˆ§
        config.auxiliary_loss_config!.hidden_state_reg_enabled
    POST
        loss! â‰¥ 0 âˆ§
        CASE reg_type? OF
            L1 â†’ (* loss = mean(|hidden_states|) *)
            L2 â†’ (* loss = mean(hidden_statesÂ²) *)
            Spectral â†’ (* loss = spectral_norm(hidden_states) *)
        END

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 10: Loss Aggregation Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP ComputeTotalLoss
    Î”STATE LossFunctionState
    input?: LossInput
    output!: LossOutput
    
    PRE ValidLossInput(input?)
    POST
        ValidLossOutput(output!) âˆ§
        
        (* Total loss is weighted sum of components *)
        output!.total_loss = 
            output!.primary_loss +
            (output!.auxiliary_loss â‰  âŠ¥ ? output!.auxiliary_loss! : 0) +
            (output!.cognitive_loss â‰  âŠ¥ ? output!.cognitive_loss! : 0) âˆ§
        
        (* Perplexity computed from cross-entropy *)
        output!.perplexity = exp(output!.loss_components.cross_entropy!) âˆ§
        
        (* Statistics updated *)
        total_loss_accumulated' = total_loss_accumulated + output!.total_loss âˆ§
        steps_accumulated' = steps_accumulated + 1 âˆ§
        tokens_accumulated' = tokens_accumulated + output!.tokens_evaluated

OP ReduceLoss
    ÎSTATE LossFunctionState
    per_sample_losses?: Tensor1D[BatchSize]
    sample_weights?: optional Tensor1D[BatchSize]
    reduction?: LossReduction
    reduced_loss!: LossValue
    
    PRE 
        #per_sample_losses? > 0 âˆ§
        (sample_weights? â‰  âŠ¥ â‡’ #sample_weights?! = #per_sample_losses?)
    POST
        reduced_loss! â‰¥ 0 âˆ§
        CASE reduction? OF
            None â†’ reduced_loss! = per_sample_losses?[1]
            Mean â†’ reduced_loss! = mean(per_sample_losses?)
            Sum â†’ reduced_loss! = sum(per_sample_losses?)
            WeightedMean â†’ 
                sample_weights? â‰  âŠ¥ âˆ§
                reduced_loss! = sum(per_sample_losses? * sample_weights?!) / sum(sample_weights?!)
        END

OP ScaleLoss
    ÎSTATE LossFunctionState
    loss?: LossValue
    scaled_loss!: LossValue
    
    PRE loss? â‰¥ 0
    POST
        scaled_loss! = loss? * config.loss_scale âˆ§
        scaled_loss! â‰¥ 0

OP GetAverageLoss
    ÎSTATE LossFunctionState
    average_loss!: LossValue
    
    PRE steps_accumulated > 0
    POST
        average_loss! = total_loss_accumulated / steps_accumulated

OP ResetStatistics
    Î”STATE LossFunctionState
    
    PRE true
    POST
        total_loss_accumulated' = 0 âˆ§
        steps_accumulated' = 0 âˆ§
        tokens_accumulated' = 0 âˆ§
        component_losses_accumulated' = âŸ¦
            cross_entropy â†¦ 0,
            stream_coherence â†¦ 0,
            mode_transition â†¦ 0,
            relevance_realization â†¦ 0,
            cross_stream_attention â†¦ 0,
            nested_shell_consistency â†¦ 0,
            router_z_loss â†¦ 0,
            load_balance_loss â†¦ 0
        âŸ§ âˆ§
        expressive_loss_accumulated' = 0 âˆ§
        reflective_loss_accumulated' = 0 âˆ§
        expressive_tokens' = 0 âˆ§
        reflective_tokens' = 0

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 11: Helper Functions
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

FUNC LogSoftmax(logits: Tensor1D[VocabSize], dim: â„¤): Tensor1D[VocabSize] â‰œ
    (* log_softmax(x) = x - log(Î£ exp(x)) *)
    LET max_val = max(logits) IN
    LET shifted = logits - max_val IN
    LET log_sum_exp = log(Î£áµ¢ exp(shifted[i])) IN
    âŸ¨shifted[i] - log_sum_exp | i: 1..#logitsâŸ©

FUNC Softmax(logits: Tensor1D[VocabSize]): Tensor1D[VocabSize] â‰œ
    LET max_val = max(logits) IN
    LET exp_shifted = âŸ¨exp(logits[i] - max_val) | i: 1..#logitsâŸ© IN
    LET sum_exp = Î£áµ¢ exp_shifted[i] IN
    âŸ¨exp_shifted[i] / sum_exp | i: 1..#logitsâŸ©

FUNC Entropy(probs: Tensor1D[N]): â„ â‰œ
    -Î£áµ¢ (probs[i] > 0 ? probs[i] * log(probs[i]) : 0)

FUNC KLDivergence(p: Tensor1D[N], q: Tensor1D[N]): â„ â‰œ
    Î£áµ¢ (p[i] > 0 ? p[i] * log(p[i] / q[i]) : 0)

FUNC GetTriadForStep(step: StepId): {1, 2, 3, 4} â‰œ
    CASE step OF
        1, 5, 9 â†’ 1
        2, 6, 10 â†’ 2
        3, 7, 11 â†’ 3
        4, 8, 12 â†’ 4
    END

FUNC GetModeForStep(step: StepId): CognitiveMode â‰œ
    IF step âˆˆ {1, 2, 3, 4, 5, 8, 9} THEN Expressive
    ELSE Reflective

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 12: Theorems and Properties
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

THEOREM LossNonNegativity:
    âˆ€ input: LossInput; output: LossOutput; state: LossFunctionState â€¢
        ValidLossInput(input) âˆ§ ComputeTotalLoss(state, input, output) â‡’
        output.total_loss â‰¥ 0

THEOREM PerplexityBound:
    âˆ€ output: LossOutput â€¢
        ValidLossOutput(output) â‡’
        output.perplexity â‰¥ 1

THEOREM LossDecomposition:
    âˆ€ output: LossOutput â€¢
        ValidLossOutput(output) â‡’
        output.total_loss = output.primary_loss + 
            (output.auxiliary_loss â‰  âŠ¥ ? output.auxiliary_loss! : 0) +
            (output.cognitive_loss â‰  âŠ¥ ? output.cognitive_loss! : 0)

THEOREM CognitiveModeConsistency:
    âˆ€ state: LossFunctionState â€¢
        state.expressive_tokens + state.reflective_tokens â‰¤ state.tokens_accumulated

THEOREM ReductionConsistency:
    âˆ€ losses: Tensor1D[N]; weights: Tensor1D[N] â€¢
        (âˆ€ w âˆˆ ran weights â€¢ w > 0) â‡’
        ReduceLoss(losses, weights, WeightedMean) â‰¤ max(losses)

THEOREM LabelSmoothingBound:
    âˆ€ logits: Tensor3D[B, S, V]; labels: Tensor2D[B, S]; Îµ: â„ â€¢
        0 â‰¤ Îµ < 1 â‡’
        ComputeLabelSmoothedLoss(logits, labels, Îµ) â‰¤ 
            ComputeCrossEntropyLoss(logits, labels) + Îµ * log(V)

END LossFunction
