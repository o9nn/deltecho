(*
 * DataLoader.zpp - Data Loading Contracts for Training Pipeline
 * Z++ Formal Specification for Deltecho Model Package
 * 
 * Defines the formal contracts for data loading, batching, sampling,
 * and cognitive-aware data preparation for the training pipeline.
 *)

SCHEMA DataLoader
IMPORTS Types, TokenizerConfig, Tokenizer

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 1: Dataset Type Definitions
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE DatasetId == seqâ‚ CHAR            (* Non-empty dataset identifier *)
TYPE SampleId == â„•                     (* Sample index in dataset *)
TYPE EpochId == â„•                      (* Training epoch number *)
TYPE ShardId == â„•                      (* Data shard identifier *)

TYPE DataFormat ::=
    TextPlain |                        (* Raw text files *)
    JsonLines |                        (* JSON Lines format *)
    Parquet |                          (* Apache Parquet *)
    Arrow |                            (* Apache Arrow *)
    TokenizedBinary                    (* Pre-tokenized binary *)

TYPE SamplingStrategy ::=
    Sequential |                       (* In-order sampling *)
    RandomShuffle |                    (* Random permutation *)
    WeightedSampling |                 (* Probability-weighted *)
    CurriculumLearning |               (* Difficulty-ordered *)
    CognitiveBalanced                  (* Balance cognitive modes *)

TYPE PackingStrategy ::=
    NoPacking |                        (* One sample per sequence *)
    ConcatPacking |                    (* Concatenate with separator *)
    BestFitPacking |                   (* Bin-packing optimization *)
    CognitiveAlignedPacking            (* Align to cognitive steps *)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 2: Sample and Batch Types
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE RawSample == âŸ¦
    id: SampleId,
    text: TextInput,
    metadata: optional âŸ¦
        source: seq CHAR,
        quality_score: optional Probability,
        difficulty: optional â„,
        cognitive_mode: optional CognitiveMode,
        domain: optional seq CHAR
    âŸ§
âŸ§

TYPE TokenizedSample == âŸ¦
    id: SampleId,
    input_ids: TokenSequence,
    attention_mask: AttentionMask,
    labels: TokenSequence,
    
    (* Cognitive annotations *)
    cognitive_markers: optional seq âŸ¦
        position: â„•,
        marker_type: {stream, pivot, afford, salience},
        stream_id: optional StreamId,
        step_id: optional StepId
    âŸ§,
    
    (* Training metadata *)
    loss_mask: optional seq {0, 1},
    sample_weight: optional â„
âŸ§

TYPE TrainingBatch == âŸ¦
    batch_id: â„•,
    batch_size: BatchSize,
    
    (* Tensor data *)
    input_ids: Tensor2D[BatchSize, SeqLen],
    attention_mask: Tensor2D[BatchSize, SeqLen],
    labels: Tensor2D[BatchSize, SeqLen],
    position_ids: optional Tensor2D[BatchSize, SeqLen],
    
    (* Loss masking *)
    loss_mask: optional Tensor2D[BatchSize, SeqLen],
    sample_weights: optional Tensor1D[BatchSize],
    
    (* Cognitive batch data *)
    cognitive_stream_ids: optional Tensor1D[BatchSize],
    cognitive_step_ids: optional Tensor1D[BatchSize],
    cognitive_mode_ids: optional Tensor1D[BatchSize],
    
    (* Metadata *)
    sample_ids: seq SampleId,
    total_tokens: â„•,
    padding_tokens: â„•
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 3: Dataset Configuration
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE DatasetConfig == âŸ¦
    dataset_id: DatasetId,
    data_format: DataFormat,
    data_paths: seqâ‚ seq CHAR,
    
    (* Tokenization *)
    max_seq_length: SeqLen,
    truncation: TruncationStrategy,
    padding: PaddingStrategy,
    
    (* Sampling *)
    sampling_strategy: SamplingStrategy,
    shuffle_seed: optional â„•,
    sample_weights: optional seq â„,
    
    (* Packing *)
    packing_strategy: PackingStrategy,
    pack_separator_token: optional TokenId,
    
    (* Filtering *)
    min_length: optional â„•,
    max_length: optional â„•,
    quality_threshold: optional Probability,
    
    (* Cognitive configuration *)
    cognitive_annotation: ğ”¹,
    cognitive_mode_balance: optional âŸ¦
        expressive_ratio: Probability,
        reflective_ratio: Probability
    âŸ§,
    
    (* Sharding *)
    num_shards: â„•â‚,
    shard_id: optional ShardId
âŸ§

TYPE DataLoaderConfig == âŸ¦
    batch_size: BatchSize,
    num_workers: â„•,
    prefetch_factor: â„•â‚,
    pin_memory: ğ”¹,
    drop_last: ğ”¹,
    
    (* Distributed training *)
    world_size: â„•â‚,
    rank: â„•,
    
    (* Memory management *)
    max_tokens_per_batch: optional â„•,
    dynamic_batching: ğ”¹
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 4: DataLoader State
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

STATE DataLoaderState == âŸ¦
    (* Configuration *)
    dataset_config: DatasetConfig,
    loader_config: DataLoaderConfig,
    tokenizer: TokenizerState,
    
    (* Dataset state *)
    total_samples: â„•,
    sample_indices: seq SampleId,
    current_index: â„•,
    
    (* Epoch state *)
    current_epoch: EpochId,
    samples_seen_epoch: â„•,
    samples_seen_total: â„•,
    
    (* Batch state *)
    batches_yielded: â„•,
    tokens_processed: â„•,
    
    (* Cognitive state *)
    cognitive_mode_counts: âŸ¦
        expressive: â„•,
        reflective: â„•
    âŸ§,
    
    (* Buffer state *)
    prefetch_buffer: seq TrainingBatch,
    buffer_size: â„•,
    
    (* RNG state *)
    rng_state: seq â„•
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 5: Invariant Predicates
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

PRED ValidDatasetConfig(c: DatasetConfig) â‰œ
    #c.data_paths > 0 âˆ§
    ValidSeqLen(c.max_seq_length) âˆ§
    c.num_shards > 0 âˆ§
    (c.shard_id â‰  âŠ¥ â‡’ c.shard_id! < c.num_shards) âˆ§
    (c.min_length â‰  âŠ¥ âˆ§ c.max_length â‰  âŠ¥ â‡’ c.min_length! â‰¤ c.max_length!) âˆ§
    (c.quality_threshold â‰  âŠ¥ â‡’ 0 â‰¤ c.quality_threshold! â‰¤ 1) âˆ§
    (c.cognitive_mode_balance â‰  âŠ¥ â‡’
        c.cognitive_mode_balance!.expressive_ratio + 
        c.cognitive_mode_balance!.reflective_ratio = 1)

PRED ValidDataLoaderConfig(c: DataLoaderConfig) â‰œ
    c.batch_size > 0 âˆ§
    c.prefetch_factor > 0 âˆ§
    c.world_size > 0 âˆ§
    c.rank < c.world_size

PRED ValidTokenizedSample(s: TokenizedSample, config: DatasetConfig, tok: TokenizerConfigState) â‰œ
    #s.input_ids = #s.attention_mask âˆ§
    #s.input_ids = #s.labels âˆ§
    #s.input_ids â‰¤ config.max_seq_length âˆ§
    ValidTokenSequence(s.input_ids, tok.vocab_size) âˆ§
    ValidTokenSequence(s.labels, tok.vocab_size) âˆ§
    (s.loss_mask â‰  âŠ¥ â‡’ #s.loss_mask! = #s.input_ids) âˆ§
    (s.sample_weight â‰  âŠ¥ â‡’ s.sample_weight! > 0)

PRED ValidTrainingBatch(b: TrainingBatch, config: DataLoaderConfig) â‰œ
    b.batch_size = config.batch_size âˆ§
    dimâ‚(b.input_ids) = b.batch_size âˆ§
    dimâ‚(b.attention_mask) = b.batch_size âˆ§
    dimâ‚(b.labels) = b.batch_size âˆ§
    dimâ‚‚(b.input_ids) = dimâ‚‚(b.attention_mask) âˆ§
    dimâ‚‚(b.input_ids) = dimâ‚‚(b.labels) âˆ§
    #b.sample_ids = b.batch_size âˆ§
    b.total_tokens = b.batch_size * dimâ‚‚(b.input_ids) âˆ§
    b.padding_tokens â‰¤ b.total_tokens

PRED ValidCognitiveBatch(b: TrainingBatch) â‰œ
    (b.cognitive_stream_ids â‰  âŠ¥ â‡’
        #b.cognitive_stream_ids! = b.batch_size âˆ§
        âˆ€ s âˆˆ ran b.cognitive_stream_ids! â€¢ 1 â‰¤ s â‰¤ COGNITIVE_STREAMS) âˆ§
    (b.cognitive_step_ids â‰  âŠ¥ â‡’
        #b.cognitive_step_ids! = b.batch_size âˆ§
        âˆ€ s âˆˆ ran b.cognitive_step_ids! â€¢ 1 â‰¤ s â‰¤ COGNITIVE_STEPS) âˆ§
    (b.cognitive_mode_ids â‰  âŠ¥ â‡’
        #b.cognitive_mode_ids! = b.batch_size âˆ§
        âˆ€ m âˆˆ ran b.cognitive_mode_ids! â€¢ m âˆˆ {0, 1})

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 6: State Invariants
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

INVARIANT DataLoaderStateInvariant
    âˆ€ state: DataLoaderState â€¢
        ValidDatasetConfig(state.dataset_config) âˆ§
        ValidDataLoaderConfig(state.loader_config) âˆ§
        
        (* Index bounds *)
        state.current_index â‰¤ state.total_samples âˆ§
        state.samples_seen_epoch â‰¤ state.total_samples âˆ§
        
        (* Sample indices valid *)
        #state.sample_indices = state.total_samples âˆ§
        (âˆ€ i âˆˆ ran state.sample_indices â€¢ i < state.total_samples) âˆ§
        
        (* Buffer constraints *)
        state.buffer_size = #state.prefetch_buffer âˆ§
        state.buffer_size â‰¤ state.loader_config.prefetch_factor âˆ§
        
        (* Cognitive mode balance *)
        state.cognitive_mode_counts.expressive + 
        state.cognitive_mode_counts.reflective = state.samples_seen_total âˆ§
        
        (* Distributed training constraints *)
        (state.loader_config.world_size > 1 â‡’
            state.total_samples mod state.loader_config.world_size = 0)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 7: Data Loading Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP Initialize
    Î”STATE DataLoaderState
    dataset_config?: DatasetConfig
    loader_config?: DataLoaderConfig
    tokenizer?: TokenizerState
    
    PRE 
        ValidDatasetConfig(dataset_config?) âˆ§
        ValidDataLoaderConfig(loader_config?)
    POST
        dataset_config' = dataset_config? âˆ§
        loader_config' = loader_config? âˆ§
        tokenizer' = tokenizer? âˆ§
        current_epoch' = 0 âˆ§
        current_index' = 0 âˆ§
        samples_seen_epoch' = 0 âˆ§
        samples_seen_total' = 0 âˆ§
        batches_yielded' = 0 âˆ§
        tokens_processed' = 0 âˆ§
        cognitive_mode_counts' = âŸ¦expressive â†¦ 0, reflective â†¦ 0âŸ§ âˆ§
        prefetch_buffer' = âŸ¨âŸ© âˆ§
        buffer_size' = 0

OP LoadDataset
    Î”STATE DataLoaderState
    total_samples!: â„•
    
    PRE true
    POST
        total_samples! = total_samples' âˆ§
        total_samples' > 0 âˆ§
        #sample_indices' = total_samples' âˆ§
        (* Initial indices are sequential *)
        (âˆ€ i: 0..total_samples' - 1 â€¢ sample_indices'[i + 1] = i)

OP ShuffleIndices
    Î”STATE DataLoaderState
    seed?: â„•
    
    PRE total_samples > 0
    POST
        (* Indices are a permutation *)
        ran sample_indices' = ran sample_indices âˆ§
        #sample_indices' = #sample_indices âˆ§
        (* RNG state updated *)
        rng_state' â‰  rng_state

OP GetNextBatch
    Î”STATE DataLoaderState
    batch!: TrainingBatch
    
    PRE 
        current_index < total_samples âˆ§
        current_index + loader_config.batch_size â‰¤ total_samples
    POST
        ValidTrainingBatch(batch!, loader_config) âˆ§
        
        (* Index advanced *)
        current_index' = current_index + loader_config.batch_size âˆ§
        
        (* Counters updated *)
        samples_seen_epoch' = samples_seen_epoch + loader_config.batch_size âˆ§
        samples_seen_total' = samples_seen_total + loader_config.batch_size âˆ§
        batches_yielded' = batches_yielded + 1 âˆ§
        tokens_processed' = tokens_processed + batch!.total_tokens âˆ§
        
        (* Sample IDs from current indices *)
        batch!.sample_ids = âŸ¨sample_indices[current_index + i] | i: 1..loader_config.batch_sizeâŸ©

OP GetNextBatchDynamic
    Î”STATE DataLoaderState
    max_tokens?: â„•
    batch!: TrainingBatch
    actual_batch_size!: BatchSize
    
    PRE 
        current_index < total_samples âˆ§
        loader_config.dynamic_batching âˆ§
        loader_config.max_tokens_per_batch â‰  âŠ¥
    POST
        batch!.total_tokens â‰¤ max_tokens? âˆ§
        actual_batch_size! = batch!.batch_size âˆ§
        current_index' = current_index + actual_batch_size!

OP StartNewEpoch
    Î”STATE DataLoaderState
    
    PRE true
    POST
        current_epoch' = current_epoch + 1 âˆ§
        current_index' = 0 âˆ§
        samples_seen_epoch' = 0 âˆ§
        (* Shuffle if configured *)
        (dataset_config.sampling_strategy = RandomShuffle â‡’
            ran sample_indices' = ran sample_indices)

OP PrefetchBatches
    Î”STATE DataLoaderState
    num_batches?: â„•â‚
    
    PRE 
        buffer_size < loader_config.prefetch_factor âˆ§
        current_index < total_samples
    POST
        buffer_size' = min(buffer_size + num_batches?, loader_config.prefetch_factor) âˆ§
        #prefetch_buffer' = buffer_size' âˆ§
        (âˆ€ b âˆˆ ran prefetch_buffer' â€¢ ValidTrainingBatch(b, loader_config))

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 8: Tokenization Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP TokenizeSample
    ÎSTATE DataLoaderState
    raw_sample?: RawSample
    tokenized!: TokenizedSample
    
    PRE #raw_sample?.text > 0
    POST
        ValidTokenizedSample(tokenized!, dataset_config, tokenizer.config) âˆ§
        tokenized!.id = raw_sample?.id âˆ§
        
        (* Labels are shifted input_ids for causal LM *)
        (âˆ€ i: 1..#tokenized!.labels - 1 â€¢ 
            tokenized!.labels[i] = tokenized!.input_ids[i + 1]) âˆ§
        
        (* Cognitive markers if enabled *)
        (dataset_config.cognitive_annotation â‡’ 
            tokenized!.cognitive_markers â‰  âŠ¥)

OP TokenizeBatch
    ÎSTATE DataLoaderState
    raw_samples?: seq RawSample
    tokenized!: seq TokenizedSample
    
    PRE 
        #raw_samples? > 0 âˆ§
        âˆ€ s âˆˆ ran raw_samples? â€¢ #s.text > 0
    POST
        #tokenized! = #raw_samples? âˆ§
        âˆ€ i: 1..#tokenized! â€¢ 
            ValidTokenizedSample(tokenized![i], dataset_config, tokenizer.config) âˆ§
            tokenized![i].id = raw_samples?[i].id

OP PackSamples
    ÎSTATE DataLoaderState
    samples?: seq TokenizedSample
    max_length?: SeqLen
    packed!: seq TokenizedSample
    
    PRE 
        #samples? > 0 âˆ§
        dataset_config.packing_strategy â‰  NoPacking
    POST
        (* Total tokens preserved *)
        Î£áµ¢ #packed![i].input_ids â‰¥ Î£áµ¢ #samples?[i].input_ids âˆ§
        
        (* Each packed sample respects max length *)
        âˆ€ p âˆˆ ran packed! â€¢ #p.input_ids â‰¤ max_length? âˆ§
        
        (* Cognitive alignment if configured *)
        (dataset_config.packing_strategy = CognitiveAlignedPacking â‡’
            âˆ€ p âˆˆ ran packed! â€¢ 
                #p.input_ids mod COGNITIVE_STEPS = 0)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 9: Cognitive-Aware Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP AnnotateCognitiveMarkers
    ÎSTATE DataLoaderState
    sample?: TokenizedSample
    annotated!: TokenizedSample
    
    PRE dataset_config.cognitive_annotation
    POST
        annotated!.id = sample?.id âˆ§
        annotated!.input_ids = sample?.input_ids âˆ§
        annotated!.cognitive_markers â‰  âŠ¥ âˆ§
        
        (* Markers distributed across sequence *)
        #annotated!.cognitive_markers! > 0 âˆ§
        (âˆ€ m âˆˆ ran annotated!.cognitive_markers! â€¢ 
            m.position < #annotated!.input_ids)

OP BalanceCognitiveModes
    Î”STATE DataLoaderState
    batch?: seq TokenizedSample
    balanced!: seq TokenizedSample
    
    PRE 
        dataset_config.cognitive_mode_balance â‰  âŠ¥ âˆ§
        #batch? > 0
    POST
        #balanced! = #batch? âˆ§
        
        (* Mode ratio approximately matches target *)
        LET expressive_count = #{s âˆˆ ran balanced! | 
            s.cognitive_markers â‰  âŠ¥ âˆ§ 
            HasExpressiveMode(s.cognitive_markers!)} IN
        LET target_ratio = dataset_config.cognitive_mode_balance!.expressive_ratio IN
        |expressive_count / #balanced! - target_ratio| < 0.1

OP AssignCognitiveStream
    ÎSTATE DataLoaderState
    sample_index?: â„•
    stream_id!: StreamId
    step_id!: StepId
    
    PRE sample_index? < total_samples
    POST
        1 â‰¤ stream_id! â‰¤ COGNITIVE_STREAMS âˆ§
        1 â‰¤ step_id! â‰¤ COGNITIVE_STEPS âˆ§
        
        (* Deterministic assignment based on index *)
        stream_id! = ((sample_index? mod COGNITIVE_STREAMS) + 1) âˆ§
        step_id! = ((sample_index? mod COGNITIVE_STEPS) + 1)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 10: Distributed Training Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP ShardDataset
    Î”STATE DataLoaderState
    world_size?: â„•â‚
    rank?: â„•
    
    PRE 
        rank? < world_size? âˆ§
        total_samples mod world_size? = 0
    POST
        LET shard_size = total_samples div world_size? IN
        #sample_indices' = shard_size âˆ§
        total_samples' = shard_size âˆ§
        
        (* Each rank gets disjoint subset *)
        (âˆ€ i: 1..shard_size â€¢ 
            sample_indices'[i] = sample_indices[rank? * shard_size + i])

OP SyncEpochState
    Î”STATE DataLoaderState
    global_samples_seen?: â„•
    
    PRE loader_config.world_size > 1
    POST
        (* All ranks have consistent epoch state *)
        samples_seen_total' = global_samples_seen?

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 11: Helper Functions
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

FUNC HasExpressiveMode(markers: seq âŸ¦position: â„•, marker_type: {stream, pivot, afford, salience}, 
                                   stream_id: optional StreamId, step_id: optional StepIdâŸ§): ğ”¹ â‰œ
    âˆƒ m âˆˆ ran markers â€¢ m.marker_type = afford

FUNC ComputePaddingEfficiency(batch: TrainingBatch): Probability â‰œ
    1 - (batch.padding_tokens / batch.total_tokens)

FUNC EstimateBatchTokens(samples: seq TokenizedSample, max_length: SeqLen): â„• â‰œ
    #samples * max_length

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 12: Theorems and Properties
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

THEOREM DataPreservation:
    âˆ€ state: DataLoaderState; epoch: EpochId â€¢
        (* All samples seen exactly once per epoch *)
        (state.current_epoch = epoch âˆ§ state.samples_seen_epoch = state.total_samples) â‡’
        (âˆ€ i: 0..state.total_samples - 1 â€¢ i âˆˆ ran state.sample_indices)

THEOREM ShufflePermutation:
    âˆ€ state, state': DataLoaderState â€¢
        ShuffleIndices(state, state', seed) â‡’
        ran state'.sample_indices = ran state.sample_indices

THEOREM BatchCompleteness:
    âˆ€ state: DataLoaderState; batch: TrainingBatch â€¢
        GetNextBatch(state, batch) â‡’
        #batch.sample_ids = state.loader_config.batch_size

THEOREM CognitiveModeBalance:
    âˆ€ state: DataLoaderState â€¢
        state.dataset_config.cognitive_mode_balance â‰  âŠ¥ âˆ§
        state.samples_seen_total > 100 â‡’
        LET ratio = state.cognitive_mode_counts.expressive / state.samples_seen_total IN
        |ratio - state.dataset_config.cognitive_mode_balance!.expressive_ratio| < 0.15

THEOREM DistributedConsistency:
    âˆ€ states: seq DataLoaderState â€¢
        (#states = states[1].loader_config.world_size âˆ§
         âˆ€ s âˆˆ ran states â€¢ s.current_epoch = states[1].current_epoch) â‡’
        (* Total samples across all ranks equals original dataset *)
        Î£áµ¢ states[i].total_samples = 
            states[1].total_samples * states[1].loader_config.world_size

END DataLoader
