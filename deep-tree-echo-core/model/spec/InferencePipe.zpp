(*
 * InferencePipe.zpp - End-to-End Generation Contract
 * Z++ Formal Specification for Deltecho Model Package
 * 
 * Defines the formal contracts for the complete inference pipeline,
 * from text input through tokenization, model forward pass, sampling,
 * and detokenization to final text output.
 *)

SCHEMA InferencePipe
IMPORTS Types, TokenizerConfig, ModelConfig, Tokenizer, Model

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 1: Pipeline Configuration Types
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE GenerationConfig == âŸ¦
    (* Length parameters *)
    max_new_tokens: â„•â‚,
    min_new_tokens: â„•,
    max_length: optional SeqLen,
    
    (* Sampling parameters *)
    do_sample: ğ”¹,
    temperature: Temperature,
    top_k: optional â„•â‚,
    top_p: optional Probability,
    typical_p: optional Probability,
    
    (* Beam search parameters *)
    num_beams: â„•â‚,
    num_beam_groups: â„•â‚,
    diversity_penalty: â„,
    length_penalty: â„,
    early_stopping: ğ”¹,
    
    (* Repetition control *)
    repetition_penalty: â„,
    no_repeat_ngram_size: â„•,
    encoder_repetition_penalty: â„,
    
    (* Special tokens *)
    bos_token_id: TokenId,
    eos_token_id: TokenId,
    pad_token_id: TokenId,
    
    (* Stopping criteria *)
    stop_strings: seq TextInput,
    stop_token_ids: seq TokenId,
    
    (* Cognitive generation *)
    cognitive_mode: optional CognitiveMode,
    stream_interleave: ğ”¹,
    relevance_threshold: optional Probability
âŸ§

TYPE StreamingConfig == âŸ¦
    stream_output: ğ”¹,
    chunk_size: â„•â‚,
    yield_on_token: ğ”¹,
    yield_on_sentence: ğ”¹
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 2: Pipeline Input/Output Types
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

TYPE PipelineInput == âŸ¦
    (* Text inputs *)
    prompt: TextInput,
    system_prompt: optional TextInput,
    
    (* Chat format *)
    messages: optional seq âŸ¦
        role: {system, user, assistant},
        content: TextInput
    âŸ§,
    
    (* Generation config *)
    generation_config: GenerationConfig,
    streaming_config: optional StreamingConfig,
    
    (* Cognitive parameters *)
    initial_stream: optional StreamId,
    initial_step: optional StepId
âŸ§

TYPE PipelineOutput == âŸ¦
    (* Generated text *)
    generated_text: TextInput,
    
    (* Token information *)
    input_tokens: TokenSequence,
    output_tokens: TokenSequence,
    total_tokens: â„•,
    
    (* Generation metadata *)
    finish_reason: {length, eos_token, stop_string, stop_token},
    generation_time_ms: â„•,
    tokens_per_second: â„,
    
    (* Cognitive metadata *)
    cognitive_trace: optional seq âŸ¦
        step: StepId,
        stream: StreamId,
        mode: CognitiveMode,
        phase: CognitivePhase,
        relevance_score: optional Probability
    âŸ§,
    
    (* Logprobs if requested *)
    logprobs: optional seq âŸ¦
        token: TokenId,
        logprob: LogProb,
        top_logprobs: seq âŸ¦token: TokenId, logprob: LogProbâŸ§
    âŸ§
âŸ§

TYPE StreamChunk == âŸ¦
    text: TextInput,
    token_ids: TokenSequence,
    is_final: ğ”¹,
    finish_reason: optional {length, eos_token, stop_string, stop_token},
    cognitive_state: optional âŸ¦
        step: StepId,
        stream: StreamId,
        mode: CognitiveMode
    âŸ§
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 3: Pipeline State
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

STATE PipelineState == âŸ¦
    (* Component states *)
    tokenizer: TokenizerState,
    model: ModelState,
    
    (* Generation state *)
    is_generating: ğ”¹,
    generated_tokens: TokenSequence,
    generated_length: â„•,
    
    (* Cognitive state *)
    cognitive_enabled: ğ”¹,
    current_stream: StreamId,
    current_step: StepId,
    cognitive_history: seq âŸ¦
        step: StepId,
        stream: StreamId,
        mode: CognitiveMode,
        phase: CognitivePhase
    âŸ§,
    
    (* Stopping state *)
    should_stop: ğ”¹,
    stop_reason: optional {length, eos_token, stop_string, stop_token}
âŸ§

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 4: Invariant Predicates
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

PRED ValidGenerationConfig(g: GenerationConfig, m: ModelConfigState, t: TokenizerConfigState) â‰œ
    (* Length constraints *)
    g.max_new_tokens > 0 âˆ§
    g.min_new_tokens â‰¤ g.max_new_tokens âˆ§
    (g.max_length â‰  âŠ¥ â‡’ g.max_length! â‰¤ m.max_position_embeddings) âˆ§
    
    (* Sampling constraints *)
    g.temperature > 0 âˆ§
    (g.top_k â‰  âŠ¥ â‡’ g.top_k! > 0 âˆ§ g.top_k! â‰¤ t.vocab_size) âˆ§
    (g.top_p â‰  âŠ¥ â‡’ 0 < g.top_p! â‰¤ 1) âˆ§
    (g.typical_p â‰  âŠ¥ â‡’ 0 < g.typical_p! â‰¤ 1) âˆ§
    
    (* Beam search constraints *)
    g.num_beams â‰¥ 1 âˆ§
    g.num_beam_groups â‰¥ 1 âˆ§
    g.num_beams mod g.num_beam_groups = 0 âˆ§
    
    (* Repetition constraints *)
    g.repetition_penalty > 0 âˆ§
    
    (* Token ID validity *)
    g.bos_token_id < t.vocab_size âˆ§
    g.eos_token_id < t.vocab_size âˆ§
    g.pad_token_id < t.vocab_size âˆ§
    (âˆ€ t âˆˆ ran g.stop_token_ids â€¢ t < t.vocab_size) âˆ§
    
    (* Cognitive constraints *)
    (g.relevance_threshold â‰  âŠ¥ â‡’ 0 â‰¤ g.relevance_threshold! â‰¤ 1)

PRED ValidPipelineInput(input: PipelineInput, state: PipelineState) â‰œ
    (* Must have either prompt or messages *)
    (#input.prompt > 0 âˆ¨ input.messages â‰  âŠ¥) âˆ§
    
    (* Messages must be non-empty if provided *)
    (input.messages â‰  âŠ¥ â‡’ #input.messages! > 0) âˆ§
    
    (* Generation config must be valid *)
    ValidGenerationConfig(input.generation_config, 
                         state.model.config, 
                         state.tokenizer.config) âˆ§
    
    (* Cognitive parameters valid *)
    (input.initial_stream â‰  âŠ¥ â‡’ 
        1 â‰¤ input.initial_stream! â‰¤ COGNITIVE_STREAMS) âˆ§
    (input.initial_step â‰  âŠ¥ â‡’ 
        1 â‰¤ input.initial_step! â‰¤ COGNITIVE_STEPS)

PRED ValidPipelineOutput(output: PipelineOutput, input: PipelineInput) â‰œ
    (* Generated text is non-empty unless stopped immediately *)
    (output.finish_reason â‰  length âˆ¨ #output.generated_text > 0) âˆ§
    
    (* Token counts are consistent *)
    output.total_tokens = #output.input_tokens + #output.output_tokens âˆ§
    
    (* Generation time is positive *)
    output.generation_time_ms > 0 âˆ§
    
    (* Tokens per second is consistent *)
    output.tokens_per_second = 
        (#output.output_tokens * 1000) / output.generation_time_ms âˆ§
    
    (* Cognitive trace matches output length if enabled *)
    (output.cognitive_trace â‰  âŠ¥ â‡’ 
        #output.cognitive_trace! = #output.output_tokens)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 5: State Invariants
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

INVARIANT PipelineStateInvariant
    âˆ€ state: PipelineState â€¢
        (* Generated tokens match length *)
        #state.generated_tokens = state.generated_length âˆ§
        
        (* Cognitive state consistency *)
        (state.cognitive_enabled â‡’
            1 â‰¤ state.current_stream â‰¤ COGNITIVE_STREAMS âˆ§
            1 â‰¤ state.current_step â‰¤ COGNITIVE_STEPS) âˆ§
        
        (* Stop reason set iff should_stop *)
        (state.should_stop â‡” state.stop_reason â‰  âŠ¥)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 6: Pipeline Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP Generate
    Î”STATE PipelineState
    input?: PipelineInput
    output!: PipelineOutput
    
    PRE 
        Â¬is_generating âˆ§
        ValidPipelineInput(input?, Î¸PipelineState)
    POST
        ValidPipelineOutput(output!, input?) âˆ§
        Â¬is_generating' âˆ§
        
        (* Output respects length constraints *)
        #output!.output_tokens â‰¤ input?.generation_config.max_new_tokens âˆ§
        #output!.output_tokens â‰¥ input?.generation_config.min_new_tokens âˆ§
        
        (* Finish reason is valid *)
        (output!.finish_reason = length â‡’ 
            #output!.output_tokens = input?.generation_config.max_new_tokens) âˆ§
        (output!.finish_reason = eos_token â‡’ 
            last(output!.output_tokens) = input?.generation_config.eos_token_id) âˆ§
        
        (* State is reset *)
        generated_tokens' = âŸ¨âŸ© âˆ§
        generated_length' = 0 âˆ§
        should_stop' = false âˆ§
        stop_reason' = âŠ¥

OP GenerateStreaming
    Î”STATE PipelineState
    input?: PipelineInput
    chunks!: seq StreamChunk
    
    PRE 
        Â¬is_generating âˆ§
        ValidPipelineInput(input?, Î¸PipelineState) âˆ§
        input?.streaming_config â‰  âŠ¥
    POST
        #chunks! > 0 âˆ§
        
        (* Last chunk is final *)
        last(chunks!).is_final = true âˆ§
        last(chunks!).finish_reason â‰  âŠ¥ âˆ§
        
        (* Concatenated chunks equal full output *)
        (â€áµ¢ chunks![i].text) = 
            (LET full = Generate(input?) IN full.generated_text) âˆ§
        
        (* State is reset *)
        Â¬is_generating' âˆ§
        generated_tokens' = âŸ¨âŸ©

OP GenerateStep
    Î”STATE PipelineState
    next_token!: TokenId
    chunk!: optional StreamChunk
    
    PRE 
        is_generating âˆ§
        Â¬should_stop
    POST
        (* Token is valid *)
        next_token! < tokenizer.config.vocab_size âˆ§
        
        (* Token added to sequence *)
        generated_tokens' = generated_tokens â€ âŸ¨next_token!âŸ© âˆ§
        generated_length' = generated_length + 1 âˆ§
        
        (* Cognitive state advanced *)
        (cognitive_enabled â‡’
            current_step' = ((current_step - 1 + 1) mod COGNITIVE_STEPS) + 1 âˆ§
            (current_step' mod PHASE_OFFSET = 1 â‡’
                current_stream' = ((current_stream - 1 + 1) mod COGNITIVE_STREAMS) + 1))

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 7: Stopping Criteria Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP CheckStoppingCriteria
    Î”STATE PipelineState
    config?: GenerationConfig
    
    PRE is_generating
    POST
        (* Check max length *)
        (generated_length â‰¥ config?.max_new_tokens â‡’
            should_stop' = true âˆ§ stop_reason' = length) âˆ§
        
        (* Check EOS token *)
        (generated_length > 0 âˆ§ 
         last(generated_tokens) = config?.eos_token_id â‡’
            should_stop' = true âˆ§ stop_reason' = eos_token) âˆ§
        
        (* Check stop tokens *)
        (âˆƒ t âˆˆ ran config?.stop_token_ids â€¢ 
         generated_length > 0 âˆ§ last(generated_tokens) = t â‡’
            should_stop' = true âˆ§ stop_reason' = stop_token)

OP CheckStopStrings
    Î”STATE PipelineState
    config?: GenerationConfig
    decoded_text?: TextInput
    
    PRE is_generating
    POST
        (* Check if any stop string is suffix of decoded text *)
        (âˆƒ s âˆˆ ran config?.stop_strings â€¢ 
         IsSuffix(s, decoded_text?) â‡’
            should_stop' = true âˆ§ stop_reason' = stop_string)

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 8: Cognitive Pipeline Operations
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

OP InitializeCognitiveState
    Î”STATE PipelineState
    initial_stream?: StreamId
    initial_step?: StepId
    
    PRE 
        cognitive_enabled âˆ§
        1 â‰¤ initial_stream? â‰¤ COGNITIVE_STREAMS âˆ§
        1 â‰¤ initial_step? â‰¤ COGNITIVE_STEPS
    POST
        current_stream' = initial_stream? âˆ§
        current_step' = initial_step? âˆ§
        cognitive_history' = âŸ¨âŸ©

OP RecordCognitiveStep
    Î”STATE PipelineState
    
    PRE cognitive_enabled âˆ§ is_generating
    POST
        cognitive_history' = cognitive_history â€ âŸ¨âŸ¦
            step â†¦ current_step,
            stream â†¦ current_stream,
            mode â†¦ GetCurrentMode(current_step),
            phase â†¦ GetCurrentPhase(current_stream, current_step)
        âŸ§âŸ©

OP ComputeRelevanceScore
    ÎSTATE PipelineState
    hidden_state?: Tensor2D[SeqLen, HiddenDim]
    relevance!: Probability
    
    PRE cognitive_enabled
    POST
        0 â‰¤ relevance! â‰¤ 1 âˆ§
        (* Relevance computed from cross-stream attention *)
        true

OP ApplyRelevanceGating
    Î”STATE PipelineState
    logits?: Tensor2D[BatchSize, VocabSize]
    relevance?: Probability
    threshold?: Probability
    gated_logits!: Tensor2D[BatchSize, VocabSize]
    
    PRE 
        cognitive_enabled âˆ§
        0 â‰¤ relevance? â‰¤ 1 âˆ§
        0 â‰¤ threshold? â‰¤ 1
    POST
        (* If relevance below threshold, bias toward reflection tokens *)
        (relevance? < threshold? â‡’
            gated_logits![1][PIVOT_TOKEN_ID] > logits?[1][PIVOT_TOKEN_ID])

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 9: Helper Functions
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

FUNC IsSuffix(suffix: TextInput, text: TextInput): ğ”¹ â‰œ
    #suffix â‰¤ #text âˆ§
    (âˆ€ i: 1..#suffix â€¢ suffix[i] = text[#text - #suffix + i])

FUNC FormatChatPrompt(messages: seq âŸ¦role: {system, user, assistant}, content: TextInputâŸ§,
                      template: TextInput): TextInput â‰œ
    (* Apply chat template to format messages *)
    (* Implementation depends on template format *)
    â€áµ¢ FormatMessage(messages[i], template)

FUNC FormatMessage(msg: âŸ¦role: {system, user, assistant}, content: TextInputâŸ§,
                   template: TextInput): TextInput â‰œ
    CASE msg.role OF
        system â†’ "<|system|>" â€ msg.content â€ "<|end|>"
        user â†’ "<|user|>" â€ msg.content â€ "<|end|>"
        assistant â†’ "<|assistant|>" â€ msg.content â€ "<|end|>"
    END

(* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   SECTION 10: End-to-End Theorems
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• *)

THEOREM GenerationTerminates:
    âˆ€ input: PipelineInput; state: PipelineState â€¢
        ValidPipelineInput(input, state) â‡’
        âˆƒ output: PipelineOutput â€¢ 
            Generate(input) = output âˆ§
            ValidPipelineOutput(output, input)

THEOREM StreamingConsistency:
    âˆ€ input: PipelineInput; state: PipelineState â€¢
        ValidPipelineInput(input, state) âˆ§ input.streaming_config â‰  âŠ¥ â‡’
        LET chunks = GenerateStreaming(input) IN
        LET full = Generate(input) IN
        (â€áµ¢ chunks[i].text) = full.generated_text

THEOREM CognitiveLoopContinuity:
    âˆ€ state: PipelineState â€¢
        state.cognitive_enabled â‡’
        âˆ€ i: 1..#state.cognitive_history - 1 â€¢
            LET curr = state.cognitive_history[i] IN
            LET next = state.cognitive_history[i+1] IN
            next.step = ((curr.step - 1 + 1) mod COGNITIVE_STEPS) + 1

THEOREM TokenCountInvariant:
    âˆ€ input: PipelineInput; output: PipelineOutput â€¢
        ValidPipelineOutput(output, input) â‡’
        output.total_tokens = #output.input_tokens + #output.output_tokens

THEOREM MinLengthRespected:
    âˆ€ input: PipelineInput; output: PipelineOutput â€¢
        ValidPipelineOutput(output, input) âˆ§
        output.finish_reason â‰  length â‡’
        #output.output_tokens â‰¥ input.generation_config.min_new_tokens

END InferencePipe
