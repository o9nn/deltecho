{
  "$schema": "https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig",
  "_name_or_path": "deltecho/deep-tree-echo-core",
  "_zpp_spec": {
    "version": "1.0.0",
    "spec_files": {
      "types": "spec/Types.zpp",
      "model_config": "spec/ModelConfig.zpp",
      "model": "spec/Model.zpp",
      "inference_pipe": "spec/InferencePipe.zpp"
    },
    "schema": "ModelConfigState",
    "invariants": ["ModelConfigInvariant"],
    "cognitive_architecture": {
      "streams": 3,
      "steps": 12,
      "phase_offset": 4,
      "expressive_steps": 7,
      "reflective_steps": 5,
      "nest_levels": 4,
      "nest_terms": [1, 2, 4, 9],
      "triads": {
        "triad_1": [1, 5, 9],
        "triad_2": [2, 6, 10],
        "triad_3": [3, 7, 11],
        "triad_4": [4, 8, 12]
      }
    }
  },
  "architectures": ["DeltechoTransformer"],
  "model_type": "deltecho",
  "vocab_size": 32000,
  "hidden_size": 4096,
  "intermediate_size": 14336,
  "num_hidden_layers": 32,
  "num_attention_heads": 32,
  "num_key_value_heads": 8,
  "head_dim": 128,
  "hidden_act": "silu",
  "max_position_embeddings": 4096,
  "initializer_range": 0.02,
  "rms_norm_eps": 1e-5,
  "use_cache": true,
  "tie_word_embeddings": false,
  "rope_theta": 10000.0,
  "rope_scaling": null,
  "attention_bias": false,
  "attention_dropout": 0.0,
  "mlp_bias": false,
  "bos_token_id": 2,
  "eos_token_id": 3,
  "pad_token_id": 0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.0",
  "cognitive_config": {
    "_zpp_schema": "CognitiveConfig",
    "num_streams": 3,
    "stream_hidden_dim": 1024,
    "stream_interleave_layers": [8, 16, 24],
    "cognitive_steps": 12,
    "phase_offset": 4,
    "expressive_steps": 7,
    "reflective_steps": 5,
    "nest_levels": 4,
    "nest_terms": [1, 2, 4, 9],
    "memory_attention": true,
    "memory_slots": 256,
    "memory_dim": 1024,
    "cross_stream_attention": true,
    "stream_attention_heads": 4
  },
  "attention_config": {
    "_zpp_schema": "AttentionConfig",
    "attention_type": "grouped_query_attention",
    "num_attention_heads": 32,
    "num_key_value_heads": 8,
    "head_dim": 128,
    "attention_dropout": 0.0,
    "attention_bias": false,
    "sliding_window": null,
    "rope_theta": 10000.0,
    "rope_scaling": null,
    "cross_stream_attention": true,
    "stream_attention_heads": 4
  },
  "ffn_config": {
    "_zpp_schema": "FFNConfig",
    "intermediate_size": 14336,
    "activation_function": "silu",
    "ffn_dropout": 0.0,
    "ffn_bias": false,
    "is_moe": false,
    "num_experts": null,
    "num_experts_per_tok": null,
    "expert_capacity": null
  }
}
